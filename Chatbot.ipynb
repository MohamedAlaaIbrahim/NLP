{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0209380",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed0bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import matplotlib as plt\n",
    "import tarfile\n",
    "from googletrans import Translator\n",
    "from deep_translator import GoogleTranslator\n",
    "import filecmp\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import qalsadi.lemmatizer\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D,MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3db8ab",
   "metadata": {},
   "source": [
    "# Translating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a7ccc",
   "metadata": {},
   "source": [
    "## Loading the unprocessed data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68fae54e",
   "metadata": {},
   "source": [
    "data_url = 'https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz'\n",
    "dataset = wget.download(data_url)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a55e6955",
   "metadata": {},
   "source": [
    "tar = tarfile.open(dataset, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b80dd4e",
   "metadata": {},
   "source": [
    "train_path = 'empatheticdialogues/train.csv'\n",
    "test_path = 'empatheticdialogues/test.csv'\n",
    "valid_path = 'empatheticdialogues/valid.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5dd3c6",
   "metadata": {},
   "source": [
    "## Translating and processing the train dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06d41c7b",
   "metadata": {},
   "source": [
    "headers = []\n",
    "body = []\n",
    "with open('empatheticdialogues/train.csv', 'r') as reader:\n",
    "    lines = reader.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if(count%10000 ==0):\n",
    "            print(\"Finished \"+str(count)+ \"lines\")\n",
    "        if(count == 0):\n",
    "            headers.append(line.split(','))\n",
    "        else:\n",
    "            body.append(line.split(',')[:8])\n",
    "\n",
    "        count += 1\n",
    "\n",
    "train_df = pd.DataFrame(data = body,columns = headers[0])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebe96d0c",
   "metadata": {},
   "source": [
    "proccessed_train = pd.DataFrame(data=None, columns=['Context','Response', 'Text', 'ID'], dtype=None, copy=False) \n",
    "proccessed_train['ID'] = proccessed_train.index + 1\n",
    "proccessed_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0e3ade8",
   "metadata": {},
   "source": [
    "train_df['ID'] = train_df.index "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c4df011",
   "metadata": {},
   "source": [
    "i = 0\n",
    "end = len(train_df.index)\n",
    "while i <= end:\n",
    "    text = train_df['conv_id'][i]\n",
    "    response = train_df['conv_id'][i+1]\n",
    "    if text == response:\n",
    "        i = i + 2\n",
    "    else:\n",
    "        train_df = train_df.drop(i)\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        end = len(train_df.index)\n",
    "\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51112b8e",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in train_df.index:\n",
    "    if(x % 2) == 0:\n",
    "        new_row = {'Context':train_df['context'][x], 'Response':train_df['utterance'][x+1], 'Text':train_df['utterance'][x], 'ID':i}\n",
    "        proccessed_train = proccessed_train.append(new_row, ignore_index=True)\n",
    "        i = i + 1\n",
    "proccessed_train        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a100725",
   "metadata": {},
   "source": [
    "proccessed_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78089646",
   "metadata": {},
   "source": [
    "for x in proccessed_train.index:\n",
    "    proccessed_train['Text'][x] = proccessed_train['Text'][x].replace('_comma_', ' ')\n",
    "    proccessed_train['Response'][x] = proccessed_train['Response'][x].replace('_comma_', ' ')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5cff015",
   "metadata": {},
   "source": [
    "proccessed_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60d4e720",
   "metadata": {},
   "source": [
    "proccessed_train.to_csv(\"empatheticdialogues/proccessed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcc36ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'وضع صعب'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Translator()\n",
    "translated = translator.translate('svízelná situace', src='cs', dest='ar')\n",
    "translated.text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30d5c5aa",
   "metadata": {},
   "source": [
    "arabic_train = proccessed_train.copy(deep=True)\n",
    "arabic_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5bdf687",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in arabic_train.index:\n",
    "    print(i)\n",
    "    try:\n",
    "        translated_text = GoogleTranslator(source='auto', target='ar').translate(arabic_train['Text'][i])\n",
    "        translated_response = GoogleTranslator(source='auto', target='ar').translate(arabic_train['Response'][i])\n",
    "        arabic_train['Text'][i] = translated_text\n",
    "        arabic_train['Response'][i] = translated_response\n",
    "        i = i + 2\n",
    "    except:\n",
    "        arabic_train['Text'][i] = 'Error'\n",
    "        arabic_train['Response'][i] = 'Error'\n",
    "        i = i + 2\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f5151",
   "metadata": {},
   "source": [
    "## Translating and processing the valid dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1426cba",
   "metadata": {},
   "source": [
    "headers = []\n",
    "body = []\n",
    "with open('empatheticdialogues/valid.csv', 'r') as reader:\n",
    "    lines = reader.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if(count%10000 ==0):\n",
    "            print(\"Finished \"+str(count)+ \"lines\")\n",
    "        if(count == 0):\n",
    "            headers.append(line.split(','))\n",
    "        else:\n",
    "            body.append(line.split(',')[:8])\n",
    "\n",
    "        count += 1\n",
    "\n",
    "valid_df = pd.DataFrame(data = body,columns = headers[0])\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c197b23f",
   "metadata": {},
   "source": [
    "proccessed_valid = pd.DataFrame(data=None, columns=['Context','Response', 'Text', 'ID'], dtype=None, copy=False) \n",
    "proccessed_valid['ID'] = proccessed_valid.index + 1\n",
    "proccessed_valid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "310487a8",
   "metadata": {},
   "source": [
    "valid_df['ID'] = valid_df.index "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a71f32a",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in valid_df.index:\n",
    "    print(i)\n",
    "    if(x % 2) == 0:\n",
    "        new_row = {'Context':valid_df['context'][x], 'Response':valid_df['utterance'][x+1], 'Text':valid_df['utterance'][x], 'ID':i}\n",
    "        proccessed_valid = proccessed_valid.append(new_row, ignore_index=True)\n",
    "        i = i + 1\n",
    "proccessed_valid        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "332cfc7b",
   "metadata": {},
   "source": [
    "for x in proccessed_valid.index:\n",
    "    proccessed_valid['Text'][x] = proccessed_valid['Text'][x].replace('_comma_', ' ')\n",
    "    proccessed_valid['Response'][x] = proccessed_valid['Response'][x].replace('_comma_', ' ')\n",
    "proccessed_valid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86b59b69",
   "metadata": {},
   "source": [
    "proccessed_valid.to_csv(\"empatheticdialogues/proccessed_valid.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "510d5eea",
   "metadata": {},
   "source": [
    "arabic_valid = proccessed_valid.copy(deep=True)\n",
    "arabic_valid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9c5f15f",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in arabic_valid.index:\n",
    "    print(i)\n",
    "    try:\n",
    "        translated_text = GoogleTranslator(source='auto', target='ar').translate(arabic_valid['Text'][i])\n",
    "        translated_response = GoogleTranslator(source='auto', target='ar').translate(arabic_valid['Response'][i])\n",
    "        translated_context = GoogleTranslator(source='auto', target='ar').translate(arabic_valid['Context'][i])\n",
    "        arabic_valid['Text'][i] = translated_text\n",
    "        arabic_valid['Response'][i] = translated_response\n",
    "        arabic_valid['Context'][i] = translated_context\n",
    "        i = i + 1\n",
    "    except:\n",
    "        arabic_valid['Text'][i] = 'Error'\n",
    "        arabic_valid['Response'][i] = 'Error'\n",
    "        arabic_valid['Context'][i] = 'Error'\n",
    "        i = i + 1\n",
    "        print('Error')\n",
    "arabic_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6de45",
   "metadata": {},
   "source": [
    "## Translating and processing the test dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d11a7dd",
   "metadata": {},
   "source": [
    "headers = []\n",
    "body = []\n",
    "with open('empatheticdialogues/test.csv', 'r', encoding=\"UTF-8\") as reader:\n",
    "    lines = reader.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if(count%10000 ==0):\n",
    "            print(\"Finished \"+str(count)+ \"lines\")\n",
    "        if(count == 0):\n",
    "            headers.append(line.split(','))\n",
    "        else:\n",
    "            body.append(line.split(',')[:8])\n",
    "\n",
    "        count += 1\n",
    "\n",
    "test_df = pd.DataFrame(data = body,columns = headers[0])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "479ef74b",
   "metadata": {},
   "source": [
    "proccessed_test = pd.DataFrame(data=None, columns=['Context','Response', 'Text', 'ID'], dtype=None, copy=False) \n",
    "proccessed_test['ID'] = proccessed_test.index + 1\n",
    "proccessed_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19aa98f9",
   "metadata": {},
   "source": [
    "valid_df['ID'] = valid_df.index "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b4bc3ea",
   "metadata": {},
   "source": [
    "i = 0\n",
    "end = len(test_df.index)\n",
    "while i <= end:\n",
    "    print(i)\n",
    "    text = test_df['conv_id'][i]\n",
    "    response = test_df['conv_id'][i+1]\n",
    "    if text == response:\n",
    "        i = i + 2\n",
    "    else:\n",
    "        test_df = test_df.drop(i)\n",
    "        test_df = test_df.reset_index(drop=True)\n",
    "        end = len(test_df.index)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "574cfcf1",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in test_df.index:\n",
    "    print(i)\n",
    "    if(x % 2) == 0:\n",
    "        new_row = {'Context':test_df['context'][x], 'Response':test_df['utterance'][x+1], 'Text':test_df['utterance'][x], 'ID':i}\n",
    "        proccessed_test = proccessed_test.append(new_row, ignore_index=True)\n",
    "        i = i + 1\n",
    "proccessed_test        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e3874cf",
   "metadata": {},
   "source": [
    "for x in proccessed_test.index:\n",
    "    proccessed_test['Text'][x] = proccessed_test['Text'][x].replace('_comma_', ' ')\n",
    "    proccessed_test['Response'][x] = proccessed_test['Response'][x].replace('_comma_', ' ')\n",
    "proccessed_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3103363c",
   "metadata": {},
   "source": [
    "proccessed_test.to_csv(\"empatheticdialogues/proccessed_test.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d9a01ff",
   "metadata": {},
   "source": [
    "arabic_test = pd.read_csv(\"empatheticdialogues/proccessed_test.csv\")\n",
    "arabic_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59a0d5d7",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in arabic_test.index:\n",
    "    print(i)\n",
    "    try:\n",
    "        translated_text = GoogleTranslator(source='auto', target='ar').translate(arabic_test['Text'][i])\n",
    "        translated_response = GoogleTranslator(source='auto', target='ar').translate(arabic_test['Response'][i])\n",
    "        translated_context = GoogleTranslator(source='auto', target='ar').translate(arabic_test['Context'][i])\n",
    "        arabic_test['Text'][i] = translated_text\n",
    "        arabic_test['Response'][i] = translated_response\n",
    "        arabic_test['Context'][i] = translated_context\n",
    "        i = i + 1\n",
    "    except:\n",
    "        arabic_test['Text'][i] = 'Error'\n",
    "        arabic_test['Response'][i] = 'Error'\n",
    "        arabic_test['Context'][i] = 'Error'\n",
    "        i = i + 1\n",
    "        print('Error')\n",
    "arabic_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18113d",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63e7dd",
   "metadata": {},
   "source": [
    "## Read the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be18b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0lines\n",
      "Finished 10000lines\n",
      "Finished 20000lines\n",
      "Finished 30000lines\n",
      "Finished 40000lines\n"
     ]
    }
   ],
   "source": [
    "headers = []\n",
    "body = []\n",
    "with open('processed_trainandtest.csv', 'r',encoding='utf-8') as reader:\n",
    "    lines = reader.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        \n",
    "        if(count%10000 ==0):\n",
    "            print(\"Finished \"+str(count)+ \"lines\")\n",
    "        if(count == 0):\n",
    "            headers.append(line.split(','))\n",
    "        else:\n",
    "            if line!='\"\\n':\n",
    "                body.append(line.split(','))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "df = pd.DataFrame(data = body,columns = headers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e410840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "      <th>Text</th>\n",
       "      <th>ID\\n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>عاطفي</td>\n",
       "      <td>هل كان هذا الصديق الذي كنت تحبه أم مجرد أفضل ص...</td>\n",
       "      <td>أتذكر ذهابي لمشاهدة الألعاب النارية مع أعز أصد...</td>\n",
       "      <td>0\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>عاطفي</td>\n",
       "      <td>اين ذهبت؟</td>\n",
       "      <td>كان هذا أفضل صديق. اشتقت لها.</td>\n",
       "      <td>1\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>عاطفي</td>\n",
       "      <td>هل كان هذا شيئًا حدث بسبب جدال؟</td>\n",
       "      <td>لم نعد نتحدث.</td>\n",
       "      <td>2\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>خائف</td>\n",
       "      <td>أجل؟ أنا حقا لا أرى كيف</td>\n",
       "      <td>أشعر وكأنني ضرب على جدار فارغ عندما أرى الظلام</td>\n",
       "      <td>3\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>خائف</td>\n",
       "      <td>أصطدم في الواقع بجدران فارغة في كثير من الأحيا...</td>\n",
       "      <td>لا تشعر بذلك .. إنه لأمر عجيب</td>\n",
       "      <td>4\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45508</th>\n",
       "      <td>45508</td>\n",
       "      <td>ممتن</td>\n",
       "      <td>نعم ، يجب أن ينظر المزيد من الناس إلى الحياة ك...</td>\n",
       "      <td>سعيد لأنك تعتقد ذلك أيضًا!</td>\n",
       "      <td>45508\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45509</th>\n",
       "      <td>45509</td>\n",
       "      <td>مشمئز</td>\n",
       "      <td>هل اتصلت بالمبيد؟</td>\n",
       "      <td>رأيت صرصورًا ضخمًا خارج منزلي اليوم!</td>\n",
       "      <td>45509\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45510</th>\n",
       "      <td>45510</td>\n",
       "      <td>مشمئز</td>\n",
       "      <td>أعيش في تكساس حتى أعرف تلك المشاعر</td>\n",
       "      <td>ليس بعد منذ نهاية الأسبوع. نحن نعيش في تكساس ل...</td>\n",
       "      <td>45510\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45511</th>\n",
       "      <td>45511</td>\n",
       "      <td>قلق</td>\n",
       "      <td>ما هو الاختبار؟</td>\n",
       "      <td>لدي اختبار كبير يوم الاثنين أنا متوتر للغاية.</td>\n",
       "      <td>45511\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45512</th>\n",
       "      <td>45512</td>\n",
       "      <td>قلق</td>\n",
       "      <td>الكيمياء صعبة للغاية هل درست بجد؟</td>\n",
       "      <td>إنه لصف الكيمياء الخاص بي. لم أنم كثيرًا لأنني...</td>\n",
       "      <td>45512\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45513 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Context                                           Response  \\\n",
       "0          0   عاطفي  هل كان هذا الصديق الذي كنت تحبه أم مجرد أفضل ص...   \n",
       "1          1   عاطفي                                          اين ذهبت؟   \n",
       "2          2   عاطفي                    هل كان هذا شيئًا حدث بسبب جدال؟   \n",
       "3          3    خائف                            أجل؟ أنا حقا لا أرى كيف   \n",
       "4          4    خائف  أصطدم في الواقع بجدران فارغة في كثير من الأحيا...   \n",
       "...      ...     ...                                                ...   \n",
       "45508  45508    ممتن  نعم ، يجب أن ينظر المزيد من الناس إلى الحياة ك...   \n",
       "45509  45509   مشمئز                                  هل اتصلت بالمبيد؟   \n",
       "45510  45510   مشمئز                 أعيش في تكساس حتى أعرف تلك المشاعر   \n",
       "45511  45511     قلق                                    ما هو الاختبار؟   \n",
       "45512  45512     قلق                  الكيمياء صعبة للغاية هل درست بجد؟   \n",
       "\n",
       "                                                    Text     ID\\n  \n",
       "0      أتذكر ذهابي لمشاهدة الألعاب النارية مع أعز أصد...      0\\n  \n",
       "1                          كان هذا أفضل صديق. اشتقت لها.      1\\n  \n",
       "2                                          لم نعد نتحدث.      2\\n  \n",
       "3         أشعر وكأنني ضرب على جدار فارغ عندما أرى الظلام      3\\n  \n",
       "4                          لا تشعر بذلك .. إنه لأمر عجيب      4\\n  \n",
       "...                                                  ...      ...  \n",
       "45508                         سعيد لأنك تعتقد ذلك أيضًا!  45508\\n  \n",
       "45509               رأيت صرصورًا ضخمًا خارج منزلي اليوم!  45509\\n  \n",
       "45510  ليس بعد منذ نهاية الأسبوع. نحن نعيش في تكساس ل...  45510\\n  \n",
       "45511      لدي اختبار كبير يوم الاثنين أنا متوتر للغاية.  45511\\n  \n",
       "45512  إنه لصف الكيمياء الخاص بي. لم أنم كثيرًا لأنني...  45512\\n  \n",
       "\n",
       "[45513 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037549e",
   "metadata": {},
   "source": [
    "## Initialize the necessary variables for the lemmetization and check whether the lemmetization already exists or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4a8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "data = df.values.tolist()\n",
    "data_utterance_lemmetization = []\n",
    "dict_word_counts = {}\n",
    "count = 0\n",
    "regex = re.compile(r'[\\',a-z,A-Z,0-9]+')\n",
    "\n",
    "isFile = os.path.isfile('Document_Frequencies.csv') \n",
    "print(isFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb14b0b",
   "metadata": {},
   "source": [
    "## Create the lemmetization and document count files else read the files output is the variables are filled with the correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dad206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished lemmetizing 0 sentences\n",
      "finished lemmetizing 100 sentences\n",
      "finished lemmetizing 200 sentences\n",
      "finished lemmetizing 300 sentences\n",
      "finished lemmetizing 400 sentences\n",
      "finished lemmetizing 500 sentences\n",
      "finished lemmetizing 600 sentences\n",
      "finished lemmetizing 700 sentences\n",
      "finished lemmetizing 800 sentences\n",
      "finished lemmetizing 900 sentences\n",
      "finished lemmetizing 1000 sentences\n",
      "finished lemmetizing 1100 sentences\n",
      "finished lemmetizing 1200 sentences\n",
      "finished lemmetizing 1300 sentences\n",
      "finished lemmetizing 1400 sentences\n",
      "finished lemmetizing 1500 sentences\n",
      "finished lemmetizing 1600 sentences\n",
      "finished lemmetizing 1700 sentences\n",
      "finished lemmetizing 1800 sentences\n",
      "finished lemmetizing 1900 sentences\n",
      "finished lemmetizing 2000 sentences\n",
      "finished lemmetizing 2100 sentences\n",
      "finished lemmetizing 2200 sentences\n",
      "finished lemmetizing 2300 sentences\n",
      "finished lemmetizing 2400 sentences\n",
      "finished lemmetizing 2500 sentences\n",
      "finished lemmetizing 2600 sentences\n",
      "finished lemmetizing 2700 sentences\n",
      "finished lemmetizing 2800 sentences\n",
      "finished lemmetizing 2900 sentences\n",
      "finished lemmetizing 3000 sentences\n",
      "finished lemmetizing 3100 sentences\n",
      "finished lemmetizing 3200 sentences\n",
      "finished lemmetizing 3300 sentences\n",
      "finished lemmetizing 3400 sentences\n",
      "finished lemmetizing 3500 sentences\n",
      "finished lemmetizing 3600 sentences\n",
      "finished lemmetizing 3700 sentences\n",
      "finished lemmetizing 3800 sentences\n",
      "finished lemmetizing 3900 sentences\n",
      "finished lemmetizing 4000 sentences\n",
      "finished lemmetizing 4100 sentences\n",
      "finished lemmetizing 4200 sentences\n",
      "finished lemmetizing 4300 sentences\n",
      "finished lemmetizing 4400 sentences\n",
      "finished lemmetizing 4500 sentences\n",
      "finished lemmetizing 4600 sentences\n",
      "finished lemmetizing 4700 sentences\n",
      "finished lemmetizing 4800 sentences\n",
      "finished lemmetizing 4900 sentences\n",
      "finished lemmetizing 5000 sentences\n",
      "finished lemmetizing 5100 sentences\n",
      "finished lemmetizing 5200 sentences\n",
      "finished lemmetizing 5300 sentences\n",
      "finished lemmetizing 5400 sentences\n",
      "finished lemmetizing 5500 sentences\n",
      "finished lemmetizing 5600 sentences\n",
      "finished lemmetizing 5700 sentences\n",
      "finished lemmetizing 5800 sentences\n",
      "finished lemmetizing 5900 sentences\n",
      "finished lemmetizing 6000 sentences\n",
      "finished lemmetizing 6100 sentences\n",
      "finished lemmetizing 6200 sentences\n",
      "finished lemmetizing 6300 sentences\n",
      "finished lemmetizing 6400 sentences\n",
      "finished lemmetizing 6500 sentences\n",
      "finished lemmetizing 6600 sentences\n",
      "finished lemmetizing 6700 sentences\n",
      "finished lemmetizing 6800 sentences\n",
      "finished lemmetizing 6900 sentences\n",
      "finished lemmetizing 7000 sentences\n",
      "finished lemmetizing 7100 sentences\n",
      "finished lemmetizing 7200 sentences\n",
      "finished lemmetizing 7300 sentences\n",
      "finished lemmetizing 7400 sentences\n",
      "finished lemmetizing 7500 sentences\n",
      "finished lemmetizing 7600 sentences\n",
      "finished lemmetizing 7700 sentences\n",
      "finished lemmetizing 7800 sentences\n",
      "finished lemmetizing 7900 sentences\n",
      "finished lemmetizing 8000 sentences\n",
      "finished lemmetizing 8100 sentences\n",
      "finished lemmetizing 8200 sentences\n",
      "finished lemmetizing 8300 sentences\n",
      "finished lemmetizing 8400 sentences\n",
      "finished lemmetizing 8500 sentences\n",
      "finished lemmetizing 8600 sentences\n",
      "finished lemmetizing 8700 sentences\n",
      "finished lemmetizing 8800 sentences\n",
      "finished lemmetizing 8900 sentences\n",
      "finished lemmetizing 9000 sentences\n",
      "finished lemmetizing 9100 sentences\n",
      "finished lemmetizing 9200 sentences\n",
      "finished lemmetizing 9300 sentences\n",
      "finished lemmetizing 9400 sentences\n",
      "finished lemmetizing 9500 sentences\n",
      "finished lemmetizing 9600 sentences\n",
      "finished lemmetizing 9700 sentences\n",
      "finished lemmetizing 9800 sentences\n",
      "finished lemmetizing 9900 sentences\n",
      "finished lemmetizing 10000 sentences\n",
      "finished lemmetizing 10100 sentences\n",
      "finished lemmetizing 10200 sentences\n",
      "finished lemmetizing 10300 sentences\n",
      "finished lemmetizing 10400 sentences\n",
      "finished lemmetizing 10500 sentences\n",
      "finished lemmetizing 10600 sentences\n",
      "finished lemmetizing 10700 sentences\n",
      "finished lemmetizing 10800 sentences\n",
      "finished lemmetizing 10900 sentences\n",
      "finished lemmetizing 11000 sentences\n",
      "finished lemmetizing 11100 sentences\n",
      "finished lemmetizing 11200 sentences\n",
      "finished lemmetizing 11300 sentences\n",
      "finished lemmetizing 11400 sentences\n",
      "finished lemmetizing 11500 sentences\n",
      "finished lemmetizing 11600 sentences\n",
      "finished lemmetizing 11700 sentences\n",
      "finished lemmetizing 11800 sentences\n",
      "finished lemmetizing 11900 sentences\n",
      "finished lemmetizing 12000 sentences\n",
      "finished lemmetizing 12100 sentences\n",
      "finished lemmetizing 12200 sentences\n",
      "finished lemmetizing 12300 sentences\n",
      "finished lemmetizing 12400 sentences\n",
      "finished lemmetizing 12500 sentences\n",
      "finished lemmetizing 12600 sentences\n",
      "finished lemmetizing 12700 sentences\n",
      "finished lemmetizing 12800 sentences\n",
      "finished lemmetizing 12900 sentences\n",
      "finished lemmetizing 13000 sentences\n",
      "finished lemmetizing 13100 sentences\n",
      "finished lemmetizing 13200 sentences\n",
      "finished lemmetizing 13300 sentences\n",
      "finished lemmetizing 13400 sentences\n",
      "finished lemmetizing 13500 sentences\n",
      "finished lemmetizing 13600 sentences\n",
      "finished lemmetizing 13700 sentences\n",
      "finished lemmetizing 13800 sentences\n",
      "finished lemmetizing 13900 sentences\n",
      "finished lemmetizing 14000 sentences\n",
      "finished lemmetizing 14100 sentences\n",
      "finished lemmetizing 14200 sentences\n",
      "finished lemmetizing 14300 sentences\n",
      "finished lemmetizing 14400 sentences\n",
      "finished lemmetizing 14500 sentences\n",
      "finished lemmetizing 14600 sentences\n",
      "finished lemmetizing 14700 sentences\n",
      "finished lemmetizing 14800 sentences\n",
      "finished lemmetizing 14900 sentences\n",
      "finished lemmetizing 15000 sentences\n",
      "finished lemmetizing 15100 sentences\n",
      "finished lemmetizing 15200 sentences\n",
      "finished lemmetizing 15300 sentences\n",
      "finished lemmetizing 15400 sentences\n",
      "finished lemmetizing 15500 sentences\n",
      "finished lemmetizing 15600 sentences\n",
      "finished lemmetizing 15700 sentences\n",
      "finished lemmetizing 15800 sentences\n",
      "finished lemmetizing 15900 sentences\n",
      "finished lemmetizing 16000 sentences\n",
      "finished lemmetizing 16100 sentences\n",
      "finished lemmetizing 16200 sentences\n",
      "finished lemmetizing 16300 sentences\n",
      "finished lemmetizing 16400 sentences\n",
      "finished lemmetizing 16500 sentences\n",
      "finished lemmetizing 16600 sentences\n",
      "finished lemmetizing 16700 sentences\n",
      "finished lemmetizing 16800 sentences\n",
      "finished lemmetizing 16900 sentences\n",
      "finished lemmetizing 17000 sentences\n",
      "finished lemmetizing 17100 sentences\n",
      "finished lemmetizing 17200 sentences\n",
      "finished lemmetizing 17300 sentences\n",
      "finished lemmetizing 17400 sentences\n",
      "finished lemmetizing 17500 sentences\n",
      "finished lemmetizing 17600 sentences\n",
      "finished lemmetizing 17700 sentences\n",
      "finished lemmetizing 17800 sentences\n",
      "finished lemmetizing 17900 sentences\n",
      "finished lemmetizing 18000 sentences\n",
      "finished lemmetizing 18100 sentences\n",
      "finished lemmetizing 18200 sentences\n",
      "finished lemmetizing 18300 sentences\n",
      "finished lemmetizing 18400 sentences\n",
      "finished lemmetizing 18500 sentences\n",
      "finished lemmetizing 18600 sentences\n",
      "finished lemmetizing 18700 sentences\n",
      "finished lemmetizing 18800 sentences\n",
      "finished lemmetizing 18900 sentences\n",
      "finished lemmetizing 19000 sentences\n",
      "finished lemmetizing 19100 sentences\n",
      "finished lemmetizing 19200 sentences\n",
      "finished lemmetizing 19300 sentences\n",
      "finished lemmetizing 19400 sentences\n",
      "finished lemmetizing 19500 sentences\n",
      "finished lemmetizing 19600 sentences\n",
      "finished lemmetizing 19700 sentences\n",
      "finished lemmetizing 19800 sentences\n",
      "finished lemmetizing 19900 sentences\n",
      "finished lemmetizing 20000 sentences\n",
      "finished lemmetizing 20100 sentences\n",
      "finished lemmetizing 20200 sentences\n",
      "finished lemmetizing 20300 sentences\n",
      "finished lemmetizing 20400 sentences\n",
      "finished lemmetizing 20500 sentences\n",
      "finished lemmetizing 20600 sentences\n",
      "finished lemmetizing 20700 sentences\n",
      "finished lemmetizing 20800 sentences\n",
      "finished lemmetizing 20900 sentences\n",
      "finished lemmetizing 21000 sentences\n",
      "finished lemmetizing 21100 sentences\n",
      "finished lemmetizing 21200 sentences\n",
      "finished lemmetizing 21300 sentences\n",
      "finished lemmetizing 21400 sentences\n",
      "finished lemmetizing 21500 sentences\n",
      "finished lemmetizing 21600 sentences\n",
      "finished lemmetizing 21700 sentences\n",
      "finished lemmetizing 21800 sentences\n",
      "finished lemmetizing 21900 sentences\n",
      "finished lemmetizing 22000 sentences\n",
      "finished lemmetizing 22100 sentences\n",
      "finished lemmetizing 22200 sentences\n",
      "finished lemmetizing 22300 sentences\n",
      "finished lemmetizing 22400 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished lemmetizing 22500 sentences\n",
      "finished lemmetizing 22600 sentences\n",
      "finished lemmetizing 22700 sentences\n",
      "finished lemmetizing 22800 sentences\n",
      "finished lemmetizing 22900 sentences\n",
      "finished lemmetizing 23000 sentences\n",
      "finished lemmetizing 23100 sentences\n",
      "finished lemmetizing 23200 sentences\n",
      "finished lemmetizing 23300 sentences\n",
      "finished lemmetizing 23400 sentences\n",
      "finished lemmetizing 23500 sentences\n",
      "finished lemmetizing 23600 sentences\n",
      "finished lemmetizing 23700 sentences\n",
      "finished lemmetizing 23800 sentences\n",
      "finished lemmetizing 23900 sentences\n",
      "finished lemmetizing 24000 sentences\n",
      "finished lemmetizing 24100 sentences\n",
      "finished lemmetizing 24200 sentences\n",
      "finished lemmetizing 24300 sentences\n",
      "finished lemmetizing 24400 sentences\n",
      "finished lemmetizing 24500 sentences\n",
      "finished lemmetizing 24600 sentences\n",
      "finished lemmetizing 24700 sentences\n",
      "finished lemmetizing 24800 sentences\n",
      "finished lemmetizing 24900 sentences\n",
      "finished lemmetizing 25000 sentences\n",
      "finished lemmetizing 25100 sentences\n",
      "finished lemmetizing 25200 sentences\n",
      "finished lemmetizing 25300 sentences\n",
      "finished lemmetizing 25400 sentences\n",
      "finished lemmetizing 25500 sentences\n",
      "finished lemmetizing 25600 sentences\n",
      "finished lemmetizing 25700 sentences\n",
      "finished lemmetizing 25800 sentences\n",
      "finished lemmetizing 25900 sentences\n",
      "finished lemmetizing 26000 sentences\n",
      "finished lemmetizing 26100 sentences\n",
      "finished lemmetizing 26200 sentences\n",
      "finished lemmetizing 26300 sentences\n",
      "finished lemmetizing 26400 sentences\n",
      "finished lemmetizing 26500 sentences\n",
      "finished lemmetizing 26600 sentences\n",
      "finished lemmetizing 26700 sentences\n",
      "finished lemmetizing 26800 sentences\n",
      "finished lemmetizing 26900 sentences\n",
      "finished lemmetizing 27000 sentences\n",
      "finished lemmetizing 27100 sentences\n",
      "finished lemmetizing 27200 sentences\n",
      "finished lemmetizing 27300 sentences\n",
      "finished lemmetizing 27400 sentences\n",
      "finished lemmetizing 27500 sentences\n",
      "finished lemmetizing 27600 sentences\n",
      "finished lemmetizing 27700 sentences\n",
      "finished lemmetizing 27800 sentences\n",
      "finished lemmetizing 27900 sentences\n",
      "finished lemmetizing 28000 sentences\n",
      "finished lemmetizing 28100 sentences\n",
      "finished lemmetizing 28200 sentences\n",
      "finished lemmetizing 28300 sentences\n",
      "finished lemmetizing 28400 sentences\n",
      "finished lemmetizing 28500 sentences\n",
      "finished lemmetizing 28600 sentences\n",
      "finished lemmetizing 28700 sentences\n",
      "finished lemmetizing 28800 sentences\n",
      "finished lemmetizing 28900 sentences\n",
      "finished lemmetizing 29000 sentences\n",
      "finished lemmetizing 29100 sentences\n",
      "finished lemmetizing 29200 sentences\n",
      "finished lemmetizing 29300 sentences\n",
      "finished lemmetizing 29400 sentences\n",
      "finished lemmetizing 29500 sentences\n",
      "finished lemmetizing 29600 sentences\n",
      "finished lemmetizing 29700 sentences\n",
      "finished lemmetizing 29800 sentences\n",
      "finished lemmetizing 29900 sentences\n",
      "finished lemmetizing 30000 sentences\n",
      "finished lemmetizing 30100 sentences\n",
      "finished lemmetizing 30200 sentences\n",
      "finished lemmetizing 30300 sentences\n",
      "finished lemmetizing 30400 sentences\n",
      "finished lemmetizing 30500 sentences\n",
      "finished lemmetizing 30600 sentences\n",
      "finished lemmetizing 30700 sentences\n",
      "finished lemmetizing 30800 sentences\n",
      "finished lemmetizing 30900 sentences\n",
      "finished lemmetizing 31000 sentences\n",
      "finished lemmetizing 31100 sentences\n",
      "finished lemmetizing 31200 sentences\n",
      "finished lemmetizing 31300 sentences\n",
      "finished lemmetizing 31400 sentences\n",
      "finished lemmetizing 31500 sentences\n",
      "finished lemmetizing 31600 sentences\n",
      "finished lemmetizing 31700 sentences\n",
      "finished lemmetizing 31800 sentences\n",
      "finished lemmetizing 31900 sentences\n",
      "finished lemmetizing 32000 sentences\n",
      "finished lemmetizing 32100 sentences\n",
      "finished lemmetizing 32200 sentences\n",
      "finished lemmetizing 32300 sentences\n",
      "finished lemmetizing 32400 sentences\n",
      "finished lemmetizing 32500 sentences\n",
      "finished lemmetizing 32600 sentences\n",
      "finished lemmetizing 32700 sentences\n",
      "finished lemmetizing 32800 sentences\n",
      "finished lemmetizing 32900 sentences\n",
      "finished lemmetizing 33000 sentences\n",
      "finished lemmetizing 33100 sentences\n",
      "finished lemmetizing 33200 sentences\n",
      "finished lemmetizing 33300 sentences\n",
      "finished lemmetizing 33400 sentences\n",
      "finished lemmetizing 33500 sentences\n",
      "finished lemmetizing 33600 sentences\n",
      "finished lemmetizing 33700 sentences\n",
      "finished lemmetizing 33800 sentences\n",
      "finished lemmetizing 33900 sentences\n",
      "finished lemmetizing 34000 sentences\n",
      "finished lemmetizing 34100 sentences\n",
      "finished lemmetizing 34200 sentences\n",
      "finished lemmetizing 34300 sentences\n",
      "finished lemmetizing 34400 sentences\n",
      "finished lemmetizing 34500 sentences\n",
      "finished lemmetizing 34600 sentences\n",
      "finished lemmetizing 34700 sentences\n",
      "finished lemmetizing 34800 sentences\n",
      "finished lemmetizing 34900 sentences\n",
      "finished lemmetizing 35000 sentences\n",
      "finished lemmetizing 35100 sentences\n",
      "finished lemmetizing 35200 sentences\n",
      "finished lemmetizing 35300 sentences\n",
      "finished lemmetizing 35400 sentences\n",
      "finished lemmetizing 35500 sentences\n",
      "finished lemmetizing 35600 sentences\n",
      "finished lemmetizing 35700 sentences\n",
      "finished lemmetizing 35800 sentences\n",
      "finished lemmetizing 35900 sentences\n",
      "finished lemmetizing 36000 sentences\n",
      "finished lemmetizing 36100 sentences\n",
      "finished lemmetizing 36200 sentences\n",
      "finished lemmetizing 36300 sentences\n",
      "finished lemmetizing 36400 sentences\n",
      "finished lemmetizing 36500 sentences\n",
      "finished lemmetizing 36600 sentences\n",
      "finished lemmetizing 36700 sentences\n",
      "finished lemmetizing 36800 sentences\n",
      "finished lemmetizing 36900 sentences\n",
      "finished lemmetizing 37000 sentences\n",
      "finished lemmetizing 37100 sentences\n",
      "finished lemmetizing 37200 sentences\n",
      "finished lemmetizing 37300 sentences\n",
      "finished lemmetizing 37400 sentences\n",
      "finished lemmetizing 37500 sentences\n",
      "finished lemmetizing 37600 sentences\n",
      "finished lemmetizing 37700 sentences\n",
      "finished lemmetizing 37800 sentences\n",
      "finished lemmetizing 37900 sentences\n",
      "finished lemmetizing 38000 sentences\n",
      "finished lemmetizing 38100 sentences\n",
      "finished lemmetizing 38200 sentences\n",
      "finished lemmetizing 38300 sentences\n",
      "finished lemmetizing 38400 sentences\n",
      "finished lemmetizing 38500 sentences\n",
      "finished lemmetizing 38600 sentences\n",
      "finished lemmetizing 38700 sentences\n",
      "finished lemmetizing 38800 sentences\n",
      "finished lemmetizing 38900 sentences\n",
      "finished lemmetizing 39000 sentences\n",
      "finished lemmetizing 39100 sentences\n",
      "finished lemmetizing 39200 sentences\n",
      "finished lemmetizing 39300 sentences\n",
      "finished lemmetizing 39400 sentences\n",
      "finished lemmetizing 39500 sentences\n",
      "finished lemmetizing 39600 sentences\n",
      "finished lemmetizing 39700 sentences\n",
      "finished lemmetizing 39800 sentences\n",
      "finished lemmetizing 39900 sentences\n",
      "finished lemmetizing 40000 sentences\n",
      "finished lemmetizing 40100 sentences\n",
      "finished lemmetizing 40200 sentences\n",
      "finished lemmetizing 40300 sentences\n",
      "finished lemmetizing 40400 sentences\n",
      "finished lemmetizing 40500 sentences\n",
      "finished lemmetizing 40600 sentences\n",
      "finished lemmetizing 40700 sentences\n",
      "finished lemmetizing 40800 sentences\n",
      "finished lemmetizing 40900 sentences\n",
      "finished lemmetizing 41000 sentences\n",
      "finished lemmetizing 41100 sentences\n",
      "finished lemmetizing 41200 sentences\n",
      "finished lemmetizing 41300 sentences\n",
      "finished lemmetizing 41400 sentences\n",
      "finished lemmetizing 41500 sentences\n",
      "finished lemmetizing 41600 sentences\n",
      "finished lemmetizing 41700 sentences\n",
      "finished lemmetizing 41800 sentences\n",
      "finished lemmetizing 41900 sentences\n",
      "finished lemmetizing 42000 sentences\n",
      "finished lemmetizing 42100 sentences\n",
      "finished lemmetizing 42200 sentences\n",
      "finished lemmetizing 42300 sentences\n",
      "finished lemmetizing 42400 sentences\n",
      "finished lemmetizing 42500 sentences\n",
      "finished lemmetizing 42600 sentences\n",
      "finished lemmetizing 42700 sentences\n",
      "finished lemmetizing 42800 sentences\n",
      "finished lemmetizing 42900 sentences\n",
      "finished lemmetizing 43000 sentences\n",
      "finished lemmetizing 43100 sentences\n",
      "finished lemmetizing 43200 sentences\n",
      "finished lemmetizing 43300 sentences\n",
      "finished lemmetizing 43400 sentences\n",
      "finished lemmetizing 43500 sentences\n",
      "finished lemmetizing 43600 sentences\n",
      "finished lemmetizing 43700 sentences\n",
      "finished lemmetizing 43800 sentences\n",
      "finished lemmetizing 43900 sentences\n",
      "finished lemmetizing 44000 sentences\n",
      "finished lemmetizing 44100 sentences\n",
      "finished lemmetizing 44200 sentences\n",
      "finished lemmetizing 44300 sentences\n",
      "finished lemmetizing 44400 sentences\n",
      "finished lemmetizing 44500 sentences\n",
      "finished lemmetizing 44600 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished lemmetizing 44700 sentences\n",
      "finished lemmetizing 44800 sentences\n",
      "finished lemmetizing 44900 sentences\n",
      "finished lemmetizing 45000 sentences\n",
      "finished lemmetizing 45100 sentences\n",
      "finished lemmetizing 45200 sentences\n",
      "finished lemmetizing 45300 sentences\n",
      "finished lemmetizing 45400 sentences\n",
      "finished lemmetizing 45500 sentences\n",
      "finished writing 0 setences on the file\n",
      "finished writing 100 setences on the file\n",
      "finished writing 200 setences on the file\n",
      "finished writing 300 setences on the file\n",
      "finished writing 400 setences on the file\n",
      "finished writing 500 setences on the file\n",
      "finished writing 600 setences on the file\n",
      "finished writing 700 setences on the file\n",
      "finished writing 800 setences on the file\n",
      "finished writing 900 setences on the file\n",
      "finished writing 1000 setences on the file\n",
      "finished writing 1100 setences on the file\n",
      "finished writing 1200 setences on the file\n",
      "finished writing 1300 setences on the file\n",
      "finished writing 1400 setences on the file\n",
      "finished writing 1500 setences on the file\n",
      "finished writing 1600 setences on the file\n",
      "finished writing 1700 setences on the file\n",
      "finished writing 1800 setences on the file\n",
      "finished writing 1900 setences on the file\n",
      "finished writing 2000 setences on the file\n",
      "finished writing 2100 setences on the file\n",
      "finished writing 2200 setences on the file\n",
      "finished writing 2300 setences on the file\n",
      "finished writing 2400 setences on the file\n",
      "finished writing 2500 setences on the file\n",
      "finished writing 2600 setences on the file\n",
      "finished writing 2700 setences on the file\n",
      "finished writing 2800 setences on the file\n",
      "finished writing 2900 setences on the file\n",
      "finished writing 3000 setences on the file\n",
      "finished writing 3100 setences on the file\n",
      "finished writing 3200 setences on the file\n",
      "finished writing 3300 setences on the file\n",
      "finished writing 3400 setences on the file\n",
      "finished writing 3500 setences on the file\n",
      "finished writing 3600 setences on the file\n",
      "finished writing 3700 setences on the file\n",
      "finished writing 3800 setences on the file\n",
      "finished writing 3900 setences on the file\n",
      "finished writing 4000 setences on the file\n",
      "finished writing 4100 setences on the file\n",
      "finished writing 4200 setences on the file\n",
      "finished writing 4300 setences on the file\n",
      "finished writing 4400 setences on the file\n",
      "finished writing 4500 setences on the file\n",
      "finished writing 4600 setences on the file\n",
      "finished writing 4700 setences on the file\n",
      "finished writing 4800 setences on the file\n",
      "finished writing 4900 setences on the file\n",
      "finished writing 5000 setences on the file\n",
      "finished writing 5100 setences on the file\n",
      "finished writing 5200 setences on the file\n",
      "finished writing 5300 setences on the file\n",
      "finished writing 5400 setences on the file\n",
      "finished writing 5500 setences on the file\n",
      "finished writing 5600 setences on the file\n",
      "finished writing 5700 setences on the file\n",
      "finished writing 5800 setences on the file\n",
      "finished writing 5900 setences on the file\n",
      "finished writing 6000 setences on the file\n",
      "finished writing 6100 setences on the file\n",
      "finished writing 6200 setences on the file\n",
      "finished writing 6300 setences on the file\n",
      "finished writing 6400 setences on the file\n",
      "finished writing 6500 setences on the file\n",
      "finished writing 6600 setences on the file\n",
      "finished writing 6700 setences on the file\n",
      "finished writing 6800 setences on the file\n",
      "finished writing 6900 setences on the file\n",
      "finished writing 7000 setences on the file\n",
      "finished writing 7100 setences on the file\n",
      "finished writing 7200 setences on the file\n",
      "finished writing 7300 setences on the file\n",
      "finished writing 7400 setences on the file\n",
      "finished writing 7500 setences on the file\n",
      "finished writing 7600 setences on the file\n",
      "finished writing 7700 setences on the file\n",
      "finished writing 7800 setences on the file\n",
      "done\n",
      "finished writing 0 setences on the file\n",
      "finished writing 100 setences on the file\n",
      "finished writing 200 setences on the file\n",
      "finished writing 300 setences on the file\n",
      "finished writing 400 setences on the file\n",
      "finished writing 500 setences on the file\n",
      "finished writing 600 setences on the file\n",
      "finished writing 700 setences on the file\n",
      "finished writing 800 setences on the file\n",
      "finished writing 900 setences on the file\n",
      "finished writing 1000 setences on the file\n",
      "finished writing 1100 setences on the file\n",
      "finished writing 1200 setences on the file\n",
      "finished writing 1300 setences on the file\n",
      "finished writing 1400 setences on the file\n",
      "finished writing 1500 setences on the file\n",
      "finished writing 1600 setences on the file\n",
      "finished writing 1700 setences on the file\n",
      "finished writing 1800 setences on the file\n",
      "finished writing 1900 setences on the file\n",
      "finished writing 2000 setences on the file\n",
      "finished writing 2100 setences on the file\n",
      "finished writing 2200 setences on the file\n",
      "finished writing 2300 setences on the file\n",
      "finished writing 2400 setences on the file\n",
      "finished writing 2500 setences on the file\n",
      "finished writing 2600 setences on the file\n",
      "finished writing 2700 setences on the file\n",
      "finished writing 2800 setences on the file\n",
      "finished writing 2900 setences on the file\n",
      "finished writing 3000 setences on the file\n",
      "finished writing 3100 setences on the file\n",
      "finished writing 3200 setences on the file\n",
      "finished writing 3300 setences on the file\n",
      "finished writing 3400 setences on the file\n",
      "finished writing 3500 setences on the file\n",
      "finished writing 3600 setences on the file\n",
      "finished writing 3700 setences on the file\n",
      "finished writing 3800 setences on the file\n",
      "finished writing 3900 setences on the file\n",
      "finished writing 4000 setences on the file\n",
      "finished writing 4100 setences on the file\n",
      "finished writing 4200 setences on the file\n",
      "finished writing 4300 setences on the file\n",
      "finished writing 4400 setences on the file\n",
      "finished writing 4500 setences on the file\n",
      "finished writing 4600 setences on the file\n",
      "finished writing 4700 setences on the file\n",
      "finished writing 4800 setences on the file\n",
      "finished writing 4900 setences on the file\n",
      "finished writing 5000 setences on the file\n",
      "finished writing 5100 setences on the file\n",
      "finished writing 5200 setences on the file\n",
      "finished writing 5300 setences on the file\n",
      "finished writing 5400 setences on the file\n",
      "finished writing 5500 setences on the file\n",
      "finished writing 5600 setences on the file\n",
      "finished writing 5700 setences on the file\n",
      "finished writing 5800 setences on the file\n",
      "finished writing 5900 setences on the file\n",
      "finished writing 6000 setences on the file\n",
      "finished writing 6100 setences on the file\n",
      "finished writing 6200 setences on the file\n",
      "finished writing 6300 setences on the file\n",
      "finished writing 6400 setences on the file\n",
      "finished writing 6500 setences on the file\n",
      "finished writing 6600 setences on the file\n",
      "finished writing 6700 setences on the file\n",
      "finished writing 6800 setences on the file\n",
      "finished writing 6900 setences on the file\n",
      "finished writing 7000 setences on the file\n",
      "finished writing 7100 setences on the file\n",
      "finished writing 7200 setences on the file\n",
      "finished writing 7300 setences on the file\n",
      "finished writing 7400 setences on the file\n",
      "finished writing 7500 setences on the file\n",
      "finished writing 7600 setences on the file\n",
      "finished writing 7700 setences on the file\n",
      "finished writing 7800 setences on the file\n",
      "finished writing 7900 setences on the file\n",
      "finished writing 8000 setences on the file\n",
      "finished writing 8100 setences on the file\n",
      "finished writing 8200 setences on the file\n",
      "finished writing 8300 setences on the file\n",
      "finished writing 8400 setences on the file\n",
      "finished writing 8500 setences on the file\n",
      "finished writing 8600 setences on the file\n",
      "finished writing 8700 setences on the file\n",
      "finished writing 8800 setences on the file\n",
      "finished writing 8900 setences on the file\n",
      "finished writing 9000 setences on the file\n",
      "finished writing 9100 setences on the file\n",
      "finished writing 9200 setences on the file\n",
      "finished writing 9300 setences on the file\n",
      "finished writing 9400 setences on the file\n",
      "finished writing 9500 setences on the file\n",
      "finished writing 9600 setences on the file\n",
      "finished writing 9700 setences on the file\n",
      "finished writing 9800 setences on the file\n",
      "finished writing 9900 setences on the file\n",
      "finished writing 10000 setences on the file\n",
      "finished writing 10100 setences on the file\n",
      "finished writing 10200 setences on the file\n",
      "finished writing 10300 setences on the file\n",
      "finished writing 10400 setences on the file\n",
      "finished writing 10500 setences on the file\n",
      "finished writing 10600 setences on the file\n",
      "finished writing 10700 setences on the file\n",
      "finished writing 10800 setences on the file\n",
      "finished writing 10900 setences on the file\n",
      "finished writing 11000 setences on the file\n",
      "finished writing 11100 setences on the file\n",
      "finished writing 11200 setences on the file\n",
      "finished writing 11300 setences on the file\n",
      "finished writing 11400 setences on the file\n",
      "finished writing 11500 setences on the file\n",
      "finished writing 11600 setences on the file\n",
      "finished writing 11700 setences on the file\n",
      "finished writing 11800 setences on the file\n",
      "finished writing 11900 setences on the file\n",
      "finished writing 12000 setences on the file\n",
      "finished writing 12100 setences on the file\n",
      "finished writing 12200 setences on the file\n",
      "finished writing 12300 setences on the file\n",
      "finished writing 12400 setences on the file\n",
      "finished writing 12500 setences on the file\n",
      "finished writing 12600 setences on the file\n",
      "finished writing 12700 setences on the file\n",
      "finished writing 12800 setences on the file\n",
      "finished writing 12900 setences on the file\n",
      "finished writing 13000 setences on the file\n",
      "finished writing 13100 setences on the file\n",
      "finished writing 13200 setences on the file\n",
      "finished writing 13300 setences on the file\n",
      "finished writing 13400 setences on the file\n",
      "finished writing 13500 setences on the file\n",
      "finished writing 13600 setences on the file\n",
      "finished writing 13700 setences on the file\n",
      "finished writing 13800 setences on the file\n",
      "finished writing 13900 setences on the file\n",
      "finished writing 14000 setences on the file\n",
      "finished writing 14100 setences on the file\n",
      "finished writing 14200 setences on the file\n",
      "finished writing 14300 setences on the file\n",
      "finished writing 14400 setences on the file\n",
      "finished writing 14500 setences on the file\n",
      "finished writing 14600 setences on the file\n",
      "finished writing 14700 setences on the file\n",
      "finished writing 14800 setences on the file\n",
      "finished writing 14900 setences on the file\n",
      "finished writing 15000 setences on the file\n",
      "finished writing 15100 setences on the file\n",
      "finished writing 15200 setences on the file\n",
      "finished writing 15300 setences on the file\n",
      "finished writing 15400 setences on the file\n",
      "finished writing 15500 setences on the file\n",
      "finished writing 15600 setences on the file\n",
      "finished writing 15700 setences on the file\n",
      "finished writing 15800 setences on the file\n",
      "finished writing 15900 setences on the file\n",
      "finished writing 16000 setences on the file\n",
      "finished writing 16100 setences on the file\n",
      "finished writing 16200 setences on the file\n",
      "finished writing 16300 setences on the file\n",
      "finished writing 16400 setences on the file\n",
      "finished writing 16500 setences on the file\n",
      "finished writing 16600 setences on the file\n",
      "finished writing 16700 setences on the file\n",
      "finished writing 16800 setences on the file\n",
      "finished writing 16900 setences on the file\n",
      "finished writing 17000 setences on the file\n",
      "finished writing 17100 setences on the file\n",
      "finished writing 17200 setences on the file\n",
      "finished writing 17300 setences on the file\n",
      "finished writing 17400 setences on the file\n",
      "finished writing 17500 setences on the file\n",
      "finished writing 17600 setences on the file\n",
      "finished writing 17700 setences on the file\n",
      "finished writing 17800 setences on the file\n",
      "finished writing 17900 setences on the file\n",
      "finished writing 18000 setences on the file\n",
      "finished writing 18100 setences on the file\n",
      "finished writing 18200 setences on the file\n",
      "finished writing 18300 setences on the file\n",
      "finished writing 18400 setences on the file\n",
      "finished writing 18500 setences on the file\n",
      "finished writing 18600 setences on the file\n",
      "finished writing 18700 setences on the file\n",
      "finished writing 18800 setences on the file\n",
      "finished writing 18900 setences on the file\n",
      "finished writing 19000 setences on the file\n",
      "finished writing 19100 setences on the file\n",
      "finished writing 19200 setences on the file\n",
      "finished writing 19300 setences on the file\n",
      "finished writing 19400 setences on the file\n",
      "finished writing 19500 setences on the file\n",
      "finished writing 19600 setences on the file\n",
      "finished writing 19700 setences on the file\n",
      "finished writing 19800 setences on the file\n",
      "finished writing 19900 setences on the file\n",
      "finished writing 20000 setences on the file\n",
      "finished writing 20100 setences on the file\n",
      "finished writing 20200 setences on the file\n",
      "finished writing 20300 setences on the file\n",
      "finished writing 20400 setences on the file\n",
      "finished writing 20500 setences on the file\n",
      "finished writing 20600 setences on the file\n",
      "finished writing 20700 setences on the file\n",
      "finished writing 20800 setences on the file\n",
      "finished writing 20900 setences on the file\n",
      "finished writing 21000 setences on the file\n",
      "finished writing 21100 setences on the file\n",
      "finished writing 21200 setences on the file\n",
      "finished writing 21300 setences on the file\n",
      "finished writing 21400 setences on the file\n",
      "finished writing 21500 setences on the file\n",
      "finished writing 21600 setences on the file\n",
      "finished writing 21700 setences on the file\n",
      "finished writing 21800 setences on the file\n",
      "finished writing 21900 setences on the file\n",
      "finished writing 22000 setences on the file\n",
      "finished writing 22100 setences on the file\n",
      "finished writing 22200 setences on the file\n",
      "finished writing 22300 setences on the file\n",
      "finished writing 22400 setences on the file\n",
      "finished writing 22500 setences on the file\n",
      "finished writing 22600 setences on the file\n",
      "finished writing 22700 setences on the file\n",
      "finished writing 22800 setences on the file\n",
      "finished writing 22900 setences on the file\n",
      "finished writing 23000 setences on the file\n",
      "finished writing 23100 setences on the file\n",
      "finished writing 23200 setences on the file\n",
      "finished writing 23300 setences on the file\n",
      "finished writing 23400 setences on the file\n",
      "finished writing 23500 setences on the file\n",
      "finished writing 23600 setences on the file\n",
      "finished writing 23700 setences on the file\n",
      "finished writing 23800 setences on the file\n",
      "finished writing 23900 setences on the file\n",
      "finished writing 24000 setences on the file\n",
      "finished writing 24100 setences on the file\n",
      "finished writing 24200 setences on the file\n",
      "finished writing 24300 setences on the file\n",
      "finished writing 24400 setences on the file\n",
      "finished writing 24500 setences on the file\n",
      "finished writing 24600 setences on the file\n",
      "finished writing 24700 setences on the file\n",
      "finished writing 24800 setences on the file\n",
      "finished writing 24900 setences on the file\n",
      "finished writing 25000 setences on the file\n",
      "finished writing 25100 setences on the file\n",
      "finished writing 25200 setences on the file\n",
      "finished writing 25300 setences on the file\n",
      "finished writing 25400 setences on the file\n",
      "finished writing 25500 setences on the file\n",
      "finished writing 25600 setences on the file\n",
      "finished writing 25700 setences on the file\n",
      "finished writing 25800 setences on the file\n",
      "finished writing 25900 setences on the file\n",
      "finished writing 26000 setences on the file\n",
      "finished writing 26100 setences on the file\n",
      "finished writing 26200 setences on the file\n",
      "finished writing 26300 setences on the file\n",
      "finished writing 26400 setences on the file\n",
      "finished writing 26500 setences on the file\n",
      "finished writing 26600 setences on the file\n",
      "finished writing 26700 setences on the file\n",
      "finished writing 26800 setences on the file\n",
      "finished writing 26900 setences on the file\n",
      "finished writing 27000 setences on the file\n",
      "finished writing 27100 setences on the file\n",
      "finished writing 27200 setences on the file\n",
      "finished writing 27300 setences on the file\n",
      "finished writing 27400 setences on the file\n",
      "finished writing 27500 setences on the file\n",
      "finished writing 27600 setences on the file\n",
      "finished writing 27700 setences on the file\n",
      "finished writing 27800 setences on the file\n",
      "finished writing 27900 setences on the file\n",
      "finished writing 28000 setences on the file\n",
      "finished writing 28100 setences on the file\n",
      "finished writing 28200 setences on the file\n",
      "finished writing 28300 setences on the file\n",
      "finished writing 28400 setences on the file\n",
      "finished writing 28500 setences on the file\n",
      "finished writing 28600 setences on the file\n",
      "finished writing 28700 setences on the file\n",
      "finished writing 28800 setences on the file\n",
      "finished writing 28900 setences on the file\n",
      "finished writing 29000 setences on the file\n",
      "finished writing 29100 setences on the file\n",
      "finished writing 29200 setences on the file\n",
      "finished writing 29300 setences on the file\n",
      "finished writing 29400 setences on the file\n",
      "finished writing 29500 setences on the file\n",
      "finished writing 29600 setences on the file\n",
      "finished writing 29700 setences on the file\n",
      "finished writing 29800 setences on the file\n",
      "finished writing 29900 setences on the file\n",
      "finished writing 30000 setences on the file\n",
      "finished writing 30100 setences on the file\n",
      "finished writing 30200 setences on the file\n",
      "finished writing 30300 setences on the file\n",
      "finished writing 30400 setences on the file\n",
      "finished writing 30500 setences on the file\n",
      "finished writing 30600 setences on the file\n",
      "finished writing 30700 setences on the file\n",
      "finished writing 30800 setences on the file\n",
      "finished writing 30900 setences on the file\n",
      "finished writing 31000 setences on the file\n",
      "finished writing 31100 setences on the file\n",
      "finished writing 31200 setences on the file\n",
      "finished writing 31300 setences on the file\n",
      "finished writing 31400 setences on the file\n",
      "finished writing 31500 setences on the file\n",
      "finished writing 31600 setences on the file\n",
      "finished writing 31700 setences on the file\n",
      "finished writing 31800 setences on the file\n",
      "finished writing 31900 setences on the file\n",
      "finished writing 32000 setences on the file\n",
      "finished writing 32100 setences on the file\n",
      "finished writing 32200 setences on the file\n",
      "finished writing 32300 setences on the file\n",
      "finished writing 32400 setences on the file\n",
      "finished writing 32500 setences on the file\n",
      "finished writing 32600 setences on the file\n",
      "finished writing 32700 setences on the file\n",
      "finished writing 32800 setences on the file\n",
      "finished writing 32900 setences on the file\n",
      "finished writing 33000 setences on the file\n",
      "finished writing 33100 setences on the file\n",
      "finished writing 33200 setences on the file\n",
      "finished writing 33300 setences on the file\n",
      "finished writing 33400 setences on the file\n",
      "finished writing 33500 setences on the file\n",
      "finished writing 33600 setences on the file\n",
      "finished writing 33700 setences on the file\n",
      "finished writing 33800 setences on the file\n",
      "finished writing 33900 setences on the file\n",
      "finished writing 34000 setences on the file\n",
      "finished writing 34100 setences on the file\n",
      "finished writing 34200 setences on the file\n",
      "finished writing 34300 setences on the file\n",
      "finished writing 34400 setences on the file\n",
      "finished writing 34500 setences on the file\n",
      "finished writing 34600 setences on the file\n",
      "finished writing 34700 setences on the file\n",
      "finished writing 34800 setences on the file\n",
      "finished writing 34900 setences on the file\n",
      "finished writing 35000 setences on the file\n",
      "finished writing 35100 setences on the file\n",
      "finished writing 35200 setences on the file\n",
      "finished writing 35300 setences on the file\n",
      "finished writing 35400 setences on the file\n",
      "finished writing 35500 setences on the file\n",
      "finished writing 35600 setences on the file\n",
      "finished writing 35700 setences on the file\n",
      "finished writing 35800 setences on the file\n",
      "finished writing 35900 setences on the file\n",
      "finished writing 36000 setences on the file\n",
      "finished writing 36100 setences on the file\n",
      "finished writing 36200 setences on the file\n",
      "finished writing 36300 setences on the file\n",
      "finished writing 36400 setences on the file\n",
      "finished writing 36500 setences on the file\n",
      "finished writing 36600 setences on the file\n",
      "finished writing 36700 setences on the file\n",
      "finished writing 36800 setences on the file\n",
      "finished writing 36900 setences on the file\n",
      "finished writing 37000 setences on the file\n",
      "finished writing 37100 setences on the file\n",
      "finished writing 37200 setences on the file\n",
      "finished writing 37300 setences on the file\n",
      "finished writing 37400 setences on the file\n",
      "finished writing 37500 setences on the file\n",
      "finished writing 37600 setences on the file\n",
      "finished writing 37700 setences on the file\n",
      "finished writing 37800 setences on the file\n",
      "finished writing 37900 setences on the file\n",
      "finished writing 38000 setences on the file\n",
      "finished writing 38100 setences on the file\n",
      "finished writing 38200 setences on the file\n",
      "finished writing 38300 setences on the file\n",
      "finished writing 38400 setences on the file\n",
      "finished writing 38500 setences on the file\n",
      "finished writing 38600 setences on the file\n",
      "finished writing 38700 setences on the file\n",
      "finished writing 38800 setences on the file\n",
      "finished writing 38900 setences on the file\n",
      "finished writing 39000 setences on the file\n",
      "finished writing 39100 setences on the file\n",
      "finished writing 39200 setences on the file\n",
      "finished writing 39300 setences on the file\n",
      "finished writing 39400 setences on the file\n",
      "finished writing 39500 setences on the file\n",
      "finished writing 39600 setences on the file\n",
      "finished writing 39700 setences on the file\n",
      "finished writing 39800 setences on the file\n",
      "finished writing 39900 setences on the file\n",
      "finished writing 40000 setences on the file\n",
      "finished writing 40100 setences on the file\n",
      "finished writing 40200 setences on the file\n",
      "finished writing 40300 setences on the file\n",
      "finished writing 40400 setences on the file\n",
      "finished writing 40500 setences on the file\n",
      "finished writing 40600 setences on the file\n",
      "finished writing 40700 setences on the file\n",
      "finished writing 40800 setences on the file\n",
      "finished writing 40900 setences on the file\n",
      "finished writing 41000 setences on the file\n",
      "finished writing 41100 setences on the file\n",
      "finished writing 41200 setences on the file\n",
      "finished writing 41300 setences on the file\n",
      "finished writing 41400 setences on the file\n",
      "finished writing 41500 setences on the file\n",
      "finished writing 41600 setences on the file\n",
      "finished writing 41700 setences on the file\n",
      "finished writing 41800 setences on the file\n",
      "finished writing 41900 setences on the file\n",
      "finished writing 42000 setences on the file\n",
      "finished writing 42100 setences on the file\n",
      "finished writing 42200 setences on the file\n",
      "finished writing 42300 setences on the file\n",
      "finished writing 42400 setences on the file\n",
      "finished writing 42500 setences on the file\n",
      "finished writing 42600 setences on the file\n",
      "finished writing 42700 setences on the file\n",
      "finished writing 42800 setences on the file\n",
      "finished writing 42900 setences on the file\n",
      "finished writing 43000 setences on the file\n",
      "finished writing 43100 setences on the file\n",
      "finished writing 43200 setences on the file\n",
      "finished writing 43300 setences on the file\n",
      "finished writing 43400 setences on the file\n",
      "finished writing 43500 setences on the file\n",
      "finished writing 43600 setences on the file\n",
      "finished writing 43700 setences on the file\n",
      "finished writing 43800 setences on the file\n",
      "finished writing 43900 setences on the file\n",
      "finished writing 44000 setences on the file\n",
      "finished writing 44100 setences on the file\n",
      "finished writing 44200 setences on the file\n",
      "finished writing 44300 setences on the file\n",
      "finished writing 44400 setences on the file\n",
      "finished writing 44500 setences on the file\n",
      "finished writing 44600 setences on the file\n",
      "finished writing 44700 setences on the file\n",
      "finished writing 44800 setences on the file\n",
      "finished writing 44900 setences on the file\n",
      "finished writing 45000 setences on the file\n",
      "finished writing 45100 setences on the file\n",
      "finished writing 45200 setences on the file\n",
      "finished writing 45300 setences on the file\n",
      "finished writing 45400 setences on the file\n",
      "finished writing 45500 setences on the file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "if not isFile:\n",
    "    lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    for utterance in data:\n",
    "\n",
    "        if(count%100 ==0):\n",
    "            print('finished lemmetizing '+str(count)+' sentences')\n",
    "        count+=1\n",
    "\n",
    "        if utterance[3] != None or utterance[3] != 'Error':\n",
    "            text = regex.sub('', utterance[3])\n",
    "        else:\n",
    "            text = []\n",
    "    #     print(text)\n",
    "        lemmas = lemmer.lemmatize_text(text, return_pos=True)\n",
    "\n",
    "        lemmas = [a[0] for a in lemmas if len(a) == 2 and a[1]!=\"stopword\" and a[1]!=\"all\" and a[0] !='.']\n",
    "\n",
    "        data_utterance_lemmetization.append(lemmas)\n",
    "        for word in lemmas:\n",
    "            if word in dict_word_counts:\n",
    "                dict_word_counts[word] +=1\n",
    "            else:\n",
    "                dict_word_counts[word] = 1\n",
    "    i = 0\n",
    "    with open(\"Document_Frequencies.csv\", 'w',newline='', encoding = 'utf8') as csvfile: \n",
    "        header = ['Terms','Document_Count']\n",
    "        # creating a csv writer object \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "        csvwriter.writerow(header)\n",
    "        # writing the fields \n",
    "        for key in dict_word_counts:\n",
    "            if i%100 == 0:\n",
    "                print('finished writing '+str(i)+' setences on the file')\n",
    "            i+=1\n",
    "            row = []\n",
    "            row.append(key)\n",
    "            row.append(dict_word_counts[key])\n",
    "            csvwriter.writerow(row)\n",
    "    print(\"done\")\n",
    "    i = 0\n",
    "    with open(\"Document_Sentences.txt\", 'w',newline='', encoding = 'utf8') as file: \n",
    "        header = ['index','tokenization']\n",
    "        # creating a csv writer object \n",
    "        file.write('Index;Tokenization\\n')\n",
    "        # writing the fields \n",
    "        for sentence in data_utterance_lemmetization:\n",
    "            if i%100 == 0:\n",
    "                print('finished writing '+str(i)+' setences on the file')\n",
    "            row = str(i)+';'\n",
    "            for word in sentence:\n",
    "                row+=word+','\n",
    "\n",
    "            file.write(row+'\\n')  \n",
    "            i+=1\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"I am here\")\n",
    "    with open('Document_Frequencies.csv', 'r',encoding=\"utf8\") as reader:\n",
    "        lines = reader.readlines()\n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if(count%100 ==0):\n",
    "                print(\"Finished \"+str(count)+ \"lines\")\n",
    "            if(count != 0):\n",
    "                count_words = int(line.split(',')[1])       \n",
    "                dict_word_counts[line.split(',')[0]] = count_words\n",
    "            count += 1\n",
    "            \n",
    "    with open('Document_Sentences.txt', 'r',encoding=\"utf8\") as reader:\n",
    "        lines = reader.readlines()\n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if(count%100 ==0):\n",
    "                print(\"Finished \"+str(count)+ \"lines\")\n",
    "            if(count != 0):\n",
    "                without_line_breaks = line.replace(\"\\n\", \"\")\n",
    "                if len(line.split(';')[1])>1:\n",
    "                    without_line_breaks = without_line_breaks[:-1]\n",
    "                    data_utterance_lemmetization.append(without_line_breaks.split(';')[1].split(','))\n",
    "                else:\n",
    "                    data_utterance_lemmetization.append([])\n",
    "            count += 1\n",
    "#     LOAD FILES AND DEAL WITH THE ENCODINGS\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec7db21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تذكر',\n",
       " 'ذهاب',\n",
       " 'مشاهد',\n",
       " 'ألعاب',\n",
       " 'نار',\n",
       " 'عز',\n",
       " 'أصدقاء',\n",
       " 'مر',\n",
       " 'أولى',\n",
       " 'قضة',\n",
       " 'قت',\n",
       " 'مفرد',\n",
       " 'مع',\n",
       " 'رغم',\n",
       " 'جود',\n",
       " 'كثير',\n",
       " 'أشخاص',\n",
       " 'شعر',\n",
       " 'أشخاص',\n",
       " 'وحيد',\n",
       " 'عالم']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utterance_lemmetization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc3b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7887 45513 45513\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_word_counts),len(data_utterance_lemmetization),len(body)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7328976",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e77b6",
   "metadata": {},
   "source": [
    "## Get the total number of documents and remove the empty cells from the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f7df409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575\n",
      "44938\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x in data_utterance_lemmetization:\n",
    "    if x ==[]:\n",
    "        counter+=1\n",
    "print(counter)\n",
    "total_number_documents = len(data_utterance_lemmetization)-counter\n",
    "print(total_number_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229f583",
   "metadata": {},
   "source": [
    "## make an array of normalized tf_idf for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77a56390",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = []\n",
    "for sentence in data_utterance_lemmetization:\n",
    "    term_frequency = {}\n",
    "    tf_idf_words ={}\n",
    "    for word in sentence:\n",
    "        if word in term_frequency:\n",
    "            term_frequency[word] +=1\n",
    "        else:\n",
    "            term_frequency[word] = 1\n",
    "    for word in term_frequency:\n",
    "        tf_idf_words[word] = (1+math.log(term_frequency[word]))* (math.log(total_number_documents/dict_word_counts[word]))\n",
    "    normalization_value = 0\n",
    "    for word in tf_idf_words:\n",
    "        normalization_value += tf_idf_words[word]**2\n",
    "    normalization_value = math.sqrt(normalization_value)\n",
    "    \n",
    "    for word in tf_idf_words:\n",
    "        tf_idf_words[word] = tf_idf_words[word] / normalization_value\n",
    "        \n",
    "    tf_idf.append(tf_idf_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f178d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'تذكر': 0.1976542835521957,\n",
       " 'ذهاب': 0.17391135298222313,\n",
       " 'مشاهد': 0.21607392367260425,\n",
       " 'ألعاب': 0.23801944964516092,\n",
       " 'نار': 0.26359012981107405,\n",
       " 'عز': 0.2563546022651701,\n",
       " 'أصدقاء': 0.15991875637957617,\n",
       " 'مر': 0.17200277970310676,\n",
       " 'أولى': 0.21121329762655566,\n",
       " 'قضة': 0.2648511843372209,\n",
       " 'قت': 0.23512557811975357,\n",
       " 'مفرد': 0.23448423969932414,\n",
       " 'مع': 0.2294783401025329,\n",
       " 'رغم': 0.1821923749929021,\n",
       " 'جود': 0.2326150738485547,\n",
       " 'كثير': 0.12554184306563068,\n",
       " 'أشخاص': 0.3684801398840918,\n",
       " 'شعر': 0.10003288071103521,\n",
       " 'وحيد': 0.2444578046280264,\n",
       " 'عالم': 0.22516041841446646}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ba43a",
   "metadata": {},
   "source": [
    "## functions to process the query to the desired structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4e529c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(text):\n",
    "    lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    regex = re.compile(r'[\\',a-z,A-Z,0-9]+')\n",
    "    lemmas = lemmer.lemmatize_text(text, return_pos=True)\n",
    "    lemmas = [a[0] for a in lemmas if len(a) == 2 and a[1]!=\"stopword\" and a[1]!=\"all\" and a[0] !='.']\n",
    "    term_frequency = {}\n",
    "    tf_idf_words ={}\n",
    "    for word in lemmas:\n",
    "        if word in term_frequency:\n",
    "            term_frequency[word] +=1\n",
    "        else:\n",
    "            term_frequency[word] = 1\n",
    "            \n",
    "    for word in term_frequency:\n",
    "        tf_idf_words[word] = (1+math.log(term_frequency[word]))* (math.log(total_number_documents/dict_word_counts[word]))\n",
    "    normalization_value = 0\n",
    "    for word in tf_idf_words:\n",
    "        normalization_value += tf_idf_words[word]**2\n",
    "    normalization_value = math.sqrt(normalization_value)\n",
    "    \n",
    "    for word in tf_idf_words:\n",
    "        tf_idf_words[word] = tf_idf_words[word] / normalization_value\n",
    "    return tf_idf_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9661fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equality_context(x):\n",
    "    if x == \"معد\"or x == \"فخور\" or x == \"مخلص\"or x == \"عاطفي\" or x == \"ممتن\" or x == \"فرح\" or x == \"حنين\" or x == \"موثوق\" or x == \"الثقة\" or x == \"رعاية\" or x == \"متفائل\" or x == \"المحتوى\" or x == \"تأثرت\" or x == \"بهيجة\" or x == \"مندهش\" or x == \"توقع\":\n",
    "        return 1\n",
    "    return 0\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f34093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query,context):\n",
    "    text = process_query(query)\n",
    "    max_index = -1\n",
    "    max_similarity = 0\n",
    "    index = 0\n",
    "\n",
    "    for dictonary in tf_idf:\n",
    "        similarity = 0\n",
    "        for key in text:\n",
    "            if key in dictonary:\n",
    "                similarity+= text[key] *dictonary[key]\n",
    "        if similarity >max_similarity and equality_context(body[index][2]) == context :\n",
    "            max_index = index\n",
    "            max_similarity = similarity\n",
    "        index+=1\n",
    "    if max_index!= -1:\n",
    "        return max_similarity,body[max_index][2],body[max_index][1]\n",
    "    return 0,\"I couldn't hear you can you please say it again differently\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bd8ff4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'هل كان هذا الصديق الذي كنت تحبه أم مجرد أفضل صديق؟'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c0474",
   "metadata": {},
   "source": [
    "## test string you can copy paste the string to try the chatbot... make sure to remove the string quations ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "011c0033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body[0][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5653fb",
   "metadata": {},
   "source": [
    "# Context neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b30333",
   "metadata": {},
   "source": [
    "## Processing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0c0b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9881677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(map(lambda x: 1 if x == \"معد\"or x == \"فخور\" or \n",
    "                      x == \"مخلص\"or x == \"عاطفي\" or x == \"ممتن\" or \n",
    "                      x == \"فرح\" or x == \"حنين\" or x == \"موثوق\" or \n",
    "                      x == \"الثقة\" or x == \"رعاية\" or x == \"متفائل\" or \n",
    "                      x == \"المحتوى\" or x == \"تأثرت\" or x == \"بهيجة\" or \n",
    "                      x == \"مندهش\" or x == \"توقع\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34a76cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in range(len(data_utterance_lemmetization)):\n",
    "    text.append(' '.join(data_utterance_lemmetization[i]))\n",
    "    \n",
    "\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6e1e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45513 45513\n"
     ]
    }
   ],
   "source": [
    "print(len(y),len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "caff19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 7558\n",
    "vocab_size = 7558\n",
    "max_len = 25 \n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(text)\n",
    "text = tokenizer.texts_to_sequences(text)\n",
    "text = pad_sequences(text, padding='post', maxlen=max_len)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text, y, test_size =.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36100e",
   "metadata": {},
   "source": [
    "## Building the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd92eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(weights_path):\n",
    "    model = load_model(weights_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6962a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 25)          188950    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 15)                2460      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 32        \n",
      "=================================================================\n",
      "Total params: 191,442\n",
      "Trainable params: 191,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(layers.Embedding(max_words, 25)) #The embedding layer\n",
    "model1.add(layers.LSTM(15,dropout=0.5)) #Our LSTM layer\n",
    "model1.add(layers.Dense(2,activation='sigmoid'))\n",
    "\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "# model0 = Sequential()\n",
    "# model0.add(layers.Embedding(max_words, 15))\n",
    "# model0.add(layers.SimpleRNN(15,return_sequences=True))\n",
    "# model0.add(layers.SimpleRNN(15))\n",
    "# model0.add(layers.Dense(2,activation='softmax'))\n",
    "\n",
    "# model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b89a3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_found = os.path.isfile('best_model1.hdf5') \n",
    "\n",
    "if not model_found:\n",
    "    model1.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "    history = model1.fit(X_train, Y_train, epochs=10,validation_data=(X_test, Y_test),callbacks=[checkpoint1])\n",
    "else:\n",
    "    model1= keras.models.load_model(\"best_model1.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad4e9f",
   "metadata": {},
   "source": [
    "## Testing the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd6f2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.963968\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model1.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc52d9",
   "metadata": {},
   "source": [
    "# Integrating Similarity & NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed945879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_context_query(text):\n",
    "    lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    regex = re.compile(r'[\\',a-z,A-Z,0-9]+')\n",
    "    lemmas = lemmer.lemmatize_text(text, return_pos=True)\n",
    "    lemmas = [a[0] for a in lemmas if len(a) == 2 and a[1]!=\"stopword\" and a[1]!=\"all\" and a[0] !='.']\n",
    "    sentence = ' '.join(lemmas)\n",
    "    sentence = tokenizer.texts_to_sequences([sentence])\n",
    "    sentence = pad_sequences(sentence, padding='post', maxlen=max_len)\n",
    "    return sentence\n",
    "\n",
    "# query = process_context_query('لقد فقدت وظيفتي العام الماضي وغضبت حقًا.')  \n",
    "# y_pred = model1.predict(query)\n",
    "# query"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d88d6c0b",
   "metadata": {},
   "source": [
    "y_pred = np.argmax(y_pred)\n",
    "if(y_pred == 0):\n",
    "    print('negative emotion')\n",
    "else:\n",
    "    print('positive emotion')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07b9eb7b",
   "metadata": {},
   "source": [
    "y[14]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b87517fe",
   "metadata": {},
   "source": [
    "body[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee878e4",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42adaf",
   "metadata": {},
   "source": [
    "## run this cell for the chatbot and when you want to exit the chatbot write exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d67c047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اليوم بينما كنت أغادر للعمل في الصباح ، انفجر إطار في وسط طريق مزدحم. هذا أخاف مني الجحيم\n",
      "السياق: مشمئز\n",
      "\n",
      "\n",
      "روبوت الدردشة: أوه yikes هذا فظيع\n",
      "\n",
      "\n",
      "نعم ، أنا بخير الآن ولكن مع إصابات طفيفة\n",
      "السياق: رعاية\n",
      "\n",
      "\n",
      "روبوت الدردشة: كيف هو شعور المعصم الآن؟ أي ضرر دائم؟\n",
      "\n",
      "\n",
      "قبل بضعة أسابيع كنت أسير في الردهة أفكر في عملي الخاص عندما مدت يد فجأة من تحت طاولة وأمسكت بكاحلي. كنت مندهشا جدا. اعتقدت أنني حصلت. اتضح أنه كان ابني\n",
      "السياق: الآن\n",
      "\n",
      "\n",
      "روبوت الدردشة: نعم ، هذا وقت طويل للأسماك ، هل لديك خطط للحصول على سمكة أخرى\n",
      "\n",
      "\n",
      "ربما كنت قد أطلقت صرخة ستجعله يشكك في رجولتي لبقية حياتنا\n",
      "السياق: مذعور\n",
      "\n",
      "\n",
      "روبوت الدردشة: لم يكن ذلك مخيفًا بالنسبة لي\n",
      "\n",
      "\n",
      "خروج\n",
      "يسعدني التحدث إليكم ، وداعًا\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    query = input()\n",
    "    if query != \"خروج\":\n",
    "        processed_context_query = process_context_query(query)  \n",
    "        y_pred = model1.predict(processed_context_query)\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        similarity,answer,context = cosine_similarity(query,y_pred)\n",
    "        print('السياق:', context)\n",
    "        print('\\n')\n",
    "        print('روبوت الدردشة:', answer)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print(\"يسعدني التحدث إليكم ، وداعًا\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c4abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
