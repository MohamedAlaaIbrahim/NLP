{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0209380",
   "metadata": {},
   "source": [
    "# import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed0bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import matplotlib as plt\n",
    "import tarfile\n",
    "from googletrans import Translator\n",
    "from deep_translator import GoogleTranslator\n",
    "import filecmp\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import qalsadi.lemmatizer\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D,MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3db8ab",
   "metadata": {},
   "source": [
    "# Translating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a7ccc",
   "metadata": {},
   "source": [
    "## Loading the unprocessed data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68fae54e",
   "metadata": {},
   "source": [
    "data_url = 'https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz'\n",
    "dataset = wget.download(data_url)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a55e6955",
   "metadata": {},
   "source": [
    "tar = tarfile.open(dataset, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b80dd4e",
   "metadata": {},
   "source": [
    "train_path = 'empatheticdialogues/train.csv'\n",
    "test_path = 'empatheticdialogues/test.csv'\n",
    "valid_path = 'empatheticdialogues/valid.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5dd3c6",
   "metadata": {},
   "source": [
    "## Translating and processing the train dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06d41c7b",
   "metadata": {},
   "source": [
    "headers = []\n",
    "body = []\n",
    "with open('empatheticdialogues/train.csv', 'r') as reader:\n",
    "    lines = reader.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if(count%10000 ==0):\n",
    "            print(\"Finished \"+str(count)+ \"lines\")\n",
    "        if(count == 0):\n",
    "            headers.append(line.split(','))\n",
    "        else:\n",
    "            body.append(line.split(',')[:8])\n",
    "\n",
    "        count += 1\n",
    "\n",
    "train_df = pd.DataFrame(data = body,columns = headers[0])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebe96d0c",
   "metadata": {},
   "source": [
    "proccessed_train = pd.DataFrame(data=None, columns=['Context','Response', 'Text', 'ID'], dtype=None, copy=False) \n",
    "proccessed_train['ID'] = proccessed_train.index + 1\n",
    "proccessed_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0e3ade8",
   "metadata": {},
   "source": [
    "train_df['ID'] = train_df.index "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c4df011",
   "metadata": {},
   "source": [
    "i = 0\n",
    "end = len(train_df.index)\n",
    "while i <= end:\n",
    "    text = train_df['conv_id'][i]\n",
    "    response = train_df['conv_id'][i+1]\n",
    "    if text == response:\n",
    "        i = i + 2\n",
    "    else:\n",
    "        train_df = train_df.drop(i)\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        end = len(train_df.index)\n",
    "\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51112b8e",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in train_df.index:\n",
    "    if(x % 2) == 0:\n",
    "        new_row = {'Context':train_df['context'][x], 'Response':train_df['utterance'][x+1], 'Text':train_df['utterance'][x], 'ID':i}\n",
    "        proccessed_train = proccessed_train.append(new_row, ignore_index=True)\n",
    "        i = i + 1\n",
    "proccessed_train        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a100725",
   "metadata": {},
   "source": [
    "proccessed_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78089646",
   "metadata": {},
   "source": [
    "for x in proccessed_train.index:\n",
    "    proccessed_train['Text'][x] = proccessed_train['Text'][x].replace('_comma_', ' ')\n",
    "    proccessed_train['Response'][x] = proccessed_train['Response'][x].replace('_comma_', ' ')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5cff015",
   "metadata": {},
   "source": [
    "proccessed_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60d4e720",
   "metadata": {},
   "source": [
    "proccessed_train.to_csv(\"empatheticdialogues/proccessed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcc36ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'وضع صعب'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Translator()\n",
    "translated = translator.translate('svízelná situace', src='cs', dest='ar')\n",
    "translated.text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30d5c5aa",
   "metadata": {},
   "source": [
    "arabic_train = proccessed_train.copy(deep=True)\n",
    "arabic_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5bdf687",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in arabic_train.index:\n",
    "    print(i)\n",
    "    try:\n",
    "        translated_text = GoogleTranslator(source='auto', target='ar').translate(arabic_train['Text'][i])\n",
    "        translated_response = GoogleTranslator(source='auto', target='ar').translate(arabic_train['Response'][i])\n",
    "        arabic_train['Text'][i] = translated_text\n",
    "        arabic_train['Response'][i] = translated_response\n",
    "        i = i + 2\n",
    "    except:\n",
    "        arabic_train['Text'][i] = 'Error'\n",
    "        arabic_train['Response'][i] = 'Error'\n",
    "        i = i + 2\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f5151",
   "metadata": {},
   "source": [
    "## Translating and processing the valid dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1426cba",
   "metadata": {},
   "source": [
    "headers = []\n",
    "body = []\n",
    "with open('empatheticdialogues/valid.csv', 'r') as reader:\n",
    "    lines = reader.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if(count%10000 ==0):\n",
    "            print(\"Finished \"+str(count)+ \"lines\")\n",
    "        if(count == 0):\n",
    "            headers.append(line.split(','))\n",
    "        else:\n",
    "            body.append(line.split(',')[:8])\n",
    "\n",
    "        count += 1\n",
    "\n",
    "valid_df = pd.DataFrame(data = body,columns = headers[0])\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c197b23f",
   "metadata": {},
   "source": [
    "proccessed_valid = pd.DataFrame(data=None, columns=['Context','Response', 'Text', 'ID'], dtype=None, copy=False) \n",
    "proccessed_valid['ID'] = proccessed_valid.index + 1\n",
    "proccessed_valid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "310487a8",
   "metadata": {},
   "source": [
    "valid_df['ID'] = valid_df.index "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a71f32a",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in valid_df.index:\n",
    "    print(i)\n",
    "    if(x % 2) == 0:\n",
    "        new_row = {'Context':valid_df['context'][x], 'Response':valid_df['utterance'][x+1], 'Text':valid_df['utterance'][x], 'ID':i}\n",
    "        proccessed_valid = proccessed_valid.append(new_row, ignore_index=True)\n",
    "        i = i + 1\n",
    "proccessed_valid        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "332cfc7b",
   "metadata": {},
   "source": [
    "for x in proccessed_valid.index:\n",
    "    proccessed_valid['Text'][x] = proccessed_valid['Text'][x].replace('_comma_', ' ')\n",
    "    proccessed_valid['Response'][x] = proccessed_valid['Response'][x].replace('_comma_', ' ')\n",
    "proccessed_valid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86b59b69",
   "metadata": {},
   "source": [
    "proccessed_valid.to_csv(\"empatheticdialogues/proccessed_valid.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "510d5eea",
   "metadata": {},
   "source": [
    "arabic_valid = proccessed_valid.copy(deep=True)\n",
    "arabic_valid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9c5f15f",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in arabic_valid.index:\n",
    "    print(i)\n",
    "    try:\n",
    "        translated_text = GoogleTranslator(source='auto', target='ar').translate(arabic_valid['Text'][i])\n",
    "        translated_response = GoogleTranslator(source='auto', target='ar').translate(arabic_valid['Response'][i])\n",
    "        translated_context = GoogleTranslator(source='auto', target='ar').translate(arabic_valid['Context'][i])\n",
    "        arabic_valid['Text'][i] = translated_text\n",
    "        arabic_valid['Response'][i] = translated_response\n",
    "        arabic_valid['Context'][i] = translated_context\n",
    "        i = i + 1\n",
    "    except:\n",
    "        arabic_valid['Text'][i] = 'Error'\n",
    "        arabic_valid['Response'][i] = 'Error'\n",
    "        arabic_valid['Context'][i] = 'Error'\n",
    "        i = i + 1\n",
    "        print('Error')\n",
    "arabic_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6de45",
   "metadata": {},
   "source": [
    "## Translating and processing the test dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d11a7dd",
   "metadata": {},
   "source": [
    "headers = []\n",
    "body = []\n",
    "with open('empatheticdialogues/test.csv', 'r', encoding=\"UTF-8\") as reader:\n",
    "    lines = reader.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if(count%10000 ==0):\n",
    "            print(\"Finished \"+str(count)+ \"lines\")\n",
    "        if(count == 0):\n",
    "            headers.append(line.split(','))\n",
    "        else:\n",
    "            body.append(line.split(',')[:8])\n",
    "\n",
    "        count += 1\n",
    "\n",
    "test_df = pd.DataFrame(data = body,columns = headers[0])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "479ef74b",
   "metadata": {},
   "source": [
    "proccessed_test = pd.DataFrame(data=None, columns=['Context','Response', 'Text', 'ID'], dtype=None, copy=False) \n",
    "proccessed_test['ID'] = proccessed_test.index + 1\n",
    "proccessed_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19aa98f9",
   "metadata": {},
   "source": [
    "valid_df['ID'] = valid_df.index "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b4bc3ea",
   "metadata": {},
   "source": [
    "i = 0\n",
    "end = len(test_df.index)\n",
    "while i <= end:\n",
    "    print(i)\n",
    "    text = test_df['conv_id'][i]\n",
    "    response = test_df['conv_id'][i+1]\n",
    "    if text == response:\n",
    "        i = i + 2\n",
    "    else:\n",
    "        test_df = test_df.drop(i)\n",
    "        test_df = test_df.reset_index(drop=True)\n",
    "        end = len(test_df.index)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "574cfcf1",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in test_df.index:\n",
    "    print(i)\n",
    "    if(x % 2) == 0:\n",
    "        new_row = {'Context':test_df['context'][x], 'Response':test_df['utterance'][x+1], 'Text':test_df['utterance'][x], 'ID':i}\n",
    "        proccessed_test = proccessed_test.append(new_row, ignore_index=True)\n",
    "        i = i + 1\n",
    "proccessed_test        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e3874cf",
   "metadata": {},
   "source": [
    "for x in proccessed_test.index:\n",
    "    proccessed_test['Text'][x] = proccessed_test['Text'][x].replace('_comma_', ' ')\n",
    "    proccessed_test['Response'][x] = proccessed_test['Response'][x].replace('_comma_', ' ')\n",
    "proccessed_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3103363c",
   "metadata": {},
   "source": [
    "proccessed_test.to_csv(\"empatheticdialogues/proccessed_test.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d9a01ff",
   "metadata": {},
   "source": [
    "arabic_test = pd.read_csv(\"empatheticdialogues/proccessed_test.csv\")\n",
    "arabic_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59a0d5d7",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for x in arabic_test.index:\n",
    "    print(i)\n",
    "    try:\n",
    "        translated_text = GoogleTranslator(source='auto', target='ar').translate(arabic_test['Text'][i])\n",
    "        translated_response = GoogleTranslator(source='auto', target='ar').translate(arabic_test['Response'][i])\n",
    "        translated_context = GoogleTranslator(source='auto', target='ar').translate(arabic_test['Context'][i])\n",
    "        arabic_test['Text'][i] = translated_text\n",
    "        arabic_test['Response'][i] = translated_response\n",
    "        arabic_test['Context'][i] = translated_context\n",
    "        i = i + 1\n",
    "    except:\n",
    "        arabic_test['Text'][i] = 'Error'\n",
    "        arabic_test['Response'][i] = 'Error'\n",
    "        arabic_test['Context'][i] = 'Error'\n",
    "        i = i + 1\n",
    "        print('Error')\n",
    "arabic_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18113d",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63e7dd",
   "metadata": {},
   "source": [
    "## Read the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be18b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0lines\n",
      "Finished 10000lines\n",
      "Finished 20000lines\n",
      "Finished 30000lines\n",
      "Finished 40000lines\n",
      "Finished 50000lines\n",
      "Finished 60000lines\n",
      "Finished 70000lines\n",
      "Finished 80000lines\n"
     ]
    }
   ],
   "source": [
    "headers = []\n",
    "body = []\n",
    "with open('processed_train.csv', 'r',encoding='utf-8') as reader:\n",
    "    lines = reader.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        \n",
    "        if(count%10000 ==0):\n",
    "            print(\"Finished \"+str(count)+ \"lines\")\n",
    "        if(count == 0):\n",
    "            headers.append(line.split(','))\n",
    "        else:\n",
    "            if line!='\"\\n':\n",
    "                body.append(line.split(','))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "df = pd.DataFrame(data = body,columns = headers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e410840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "      <th>Text</th>\n",
       "      <th>\"ID\\n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>عاطفي</td>\n",
       "      <td>هل كان هذا الصديق الذي كنت تحبه أم مجرد أفضل ص...</td>\n",
       "      <td>أتذكر ذهابي لمشاهدة الألعاب النارية مع أعز أصد...</td>\n",
       "      <td>\"0\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>عاطفي</td>\n",
       "      <td>اين ذهبت؟</td>\n",
       "      <td>كان هذا أفضل صديق. اشتقت لها.</td>\n",
       "      <td>\"1\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>عاطفي</td>\n",
       "      <td>هل كان هذا شيئًا حدث بسبب جدال؟</td>\n",
       "      <td>لم نعد نتحدث.</td>\n",
       "      <td>\"2\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>خائف</td>\n",
       "      <td>أجل؟ أنا حقا لا أرى كيف</td>\n",
       "      <td>أشعر وكأنني ضرب على جدار فارغ عندما أرى الظلام</td>\n",
       "      <td>\"3\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>خائف</td>\n",
       "      <td>أصطدم في الواقع بجدران فارغة في كثير من الأحيا...</td>\n",
       "      <td>لا تشعر بذلك .. إنه لأمر عجيب</td>\n",
       "      <td>\"4\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40249</th>\n",
       "      <td>40249</td>\n",
       "      <td>40249</td>\n",
       "      <td>خائف</td>\n",
       "      <td>يا إلهي ، هذا مخيف جدًا ، أنا آسف جدًا لسماع ذلك.</td>\n",
       "      <td>اضطر منزلي الذي احترق في الليلة السابقة لإخراج...</td>\n",
       "      <td>\"40249\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40250</th>\n",
       "      <td>40250</td>\n",
       "      <td>40250</td>\n",
       "      <td>عاطفي</td>\n",
       "      <td>هل وجدت أي شيء عظيم؟</td>\n",
       "      <td>كنت أتصفح الأشياء في العلية الليلة الماضية</td>\n",
       "      <td>\"40250\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40251</th>\n",
       "      <td>40251</td>\n",
       "      <td>40251</td>\n",
       "      <td>عاطفي</td>\n",
       "      <td>يا لها من ذكرى رائعة.</td>\n",
       "      <td>نعم ، لقد وجدت بعض الصور القديمة عندما اعتدنا ...</td>\n",
       "      <td>\"40251\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40252</th>\n",
       "      <td>40252</td>\n",
       "      <td>40252</td>\n",
       "      <td>مندهش</td>\n",
       "      <td>يا هذا رائع! هذا رائع أليس كذلك؟</td>\n",
       "      <td>استيقظت هذا الصباح على زوجتي تخبرني أنها حامل!</td>\n",
       "      <td>\"40252\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40253</th>\n",
       "      <td>40253</td>\n",
       "      <td>40253</td>\n",
       "      <td>مندهش</td>\n",
       "      <td>هذا رائع!!!! تهانينا!</td>\n",
       "      <td>إنه رائع جدا. لقد كنا نريد طفلًا لفترة طويلة. ...</td>\n",
       "      <td>\"40253\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40254 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Context  \\\n",
       "0          0      0   عاطفي   \n",
       "1          1      1   عاطفي   \n",
       "2          2      2   عاطفي   \n",
       "3          3      3    خائف   \n",
       "4          4      4    خائف   \n",
       "...      ...    ...     ...   \n",
       "40249  40249  40249    خائف   \n",
       "40250  40250  40250   عاطفي   \n",
       "40251  40251  40251   عاطفي   \n",
       "40252  40252  40252   مندهش   \n",
       "40253  40253  40253   مندهش   \n",
       "\n",
       "                                                Response  \\\n",
       "0      هل كان هذا الصديق الذي كنت تحبه أم مجرد أفضل ص...   \n",
       "1                                              اين ذهبت؟   \n",
       "2                        هل كان هذا شيئًا حدث بسبب جدال؟   \n",
       "3                                أجل؟ أنا حقا لا أرى كيف   \n",
       "4      أصطدم في الواقع بجدران فارغة في كثير من الأحيا...   \n",
       "...                                                  ...   \n",
       "40249  يا إلهي ، هذا مخيف جدًا ، أنا آسف جدًا لسماع ذلك.   \n",
       "40250                               هل وجدت أي شيء عظيم؟   \n",
       "40251                              يا لها من ذكرى رائعة.   \n",
       "40252                   يا هذا رائع! هذا رائع أليس كذلك؟   \n",
       "40253                              هذا رائع!!!! تهانينا!   \n",
       "\n",
       "                                                    Text     \"ID\\n  \n",
       "0      أتذكر ذهابي لمشاهدة الألعاب النارية مع أعز أصد...      \"0\\n  \n",
       "1                          كان هذا أفضل صديق. اشتقت لها.      \"1\\n  \n",
       "2                                          لم نعد نتحدث.      \"2\\n  \n",
       "3         أشعر وكأنني ضرب على جدار فارغ عندما أرى الظلام      \"3\\n  \n",
       "4                          لا تشعر بذلك .. إنه لأمر عجيب      \"4\\n  \n",
       "...                                                  ...       ...  \n",
       "40249  اضطر منزلي الذي احترق في الليلة السابقة لإخراج...  \"40249\\n  \n",
       "40250         كنت أتصفح الأشياء في العلية الليلة الماضية  \"40250\\n  \n",
       "40251  نعم ، لقد وجدت بعض الصور القديمة عندما اعتدنا ...  \"40251\\n  \n",
       "40252     استيقظت هذا الصباح على زوجتي تخبرني أنها حامل!  \"40252\\n  \n",
       "40253  إنه رائع جدا. لقد كنا نريد طفلًا لفترة طويلة. ...  \"40253\\n  \n",
       "\n",
       "[40254 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037549e",
   "metadata": {},
   "source": [
    "## Initialize the necessary variables for the lemmetization and check whether the lemmetization already exists or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4a8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "data = df.values.tolist()\n",
    "data_utterance_lemmetization = []\n",
    "dict_word_counts = {}\n",
    "count = 0\n",
    "regex = re.compile(r'[\\',a-z,A-Z,0-9]+')\n",
    "\n",
    "isFile = os.path.isfile('Document_Frequencies.csv') \n",
    "print(isFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb14b0b",
   "metadata": {},
   "source": [
    "## Create the lemmetization and document count files else read the files output is the variables are filled with the correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dad206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here\n",
      "Finished 0lines\n",
      "Finished 100lines\n",
      "Finished 200lines\n",
      "Finished 300lines\n",
      "Finished 400lines\n",
      "Finished 500lines\n",
      "Finished 600lines\n",
      "Finished 700lines\n",
      "Finished 800lines\n",
      "Finished 900lines\n",
      "Finished 1000lines\n",
      "Finished 1100lines\n",
      "Finished 1200lines\n",
      "Finished 1300lines\n",
      "Finished 1400lines\n",
      "Finished 1500lines\n",
      "Finished 1600lines\n",
      "Finished 1700lines\n",
      "Finished 1800lines\n",
      "Finished 1900lines\n",
      "Finished 2000lines\n",
      "Finished 2100lines\n",
      "Finished 2200lines\n",
      "Finished 2300lines\n",
      "Finished 2400lines\n",
      "Finished 2500lines\n",
      "Finished 2600lines\n",
      "Finished 2700lines\n",
      "Finished 2800lines\n",
      "Finished 2900lines\n",
      "Finished 3000lines\n",
      "Finished 3100lines\n",
      "Finished 3200lines\n",
      "Finished 3300lines\n",
      "Finished 3400lines\n",
      "Finished 3500lines\n",
      "Finished 3600lines\n",
      "Finished 3700lines\n",
      "Finished 3800lines\n",
      "Finished 3900lines\n",
      "Finished 4000lines\n",
      "Finished 4100lines\n",
      "Finished 4200lines\n",
      "Finished 4300lines\n",
      "Finished 4400lines\n",
      "Finished 4500lines\n",
      "Finished 4600lines\n",
      "Finished 4700lines\n",
      "Finished 4800lines\n",
      "Finished 4900lines\n",
      "Finished 5000lines\n",
      "Finished 5100lines\n",
      "Finished 5200lines\n",
      "Finished 5300lines\n",
      "Finished 5400lines\n",
      "Finished 5500lines\n",
      "Finished 5600lines\n",
      "Finished 5700lines\n",
      "Finished 5800lines\n",
      "Finished 5900lines\n",
      "Finished 6000lines\n",
      "Finished 6100lines\n",
      "Finished 6200lines\n",
      "Finished 6300lines\n",
      "Finished 6400lines\n",
      "Finished 6500lines\n",
      "Finished 6600lines\n",
      "Finished 6700lines\n",
      "Finished 6800lines\n",
      "Finished 6900lines\n",
      "Finished 7000lines\n",
      "Finished 7100lines\n",
      "Finished 7200lines\n",
      "Finished 7300lines\n",
      "Finished 7400lines\n",
      "Finished 7500lines\n",
      "Finished 0lines\n",
      "Finished 100lines\n",
      "Finished 200lines\n",
      "Finished 300lines\n",
      "Finished 400lines\n",
      "Finished 500lines\n",
      "Finished 600lines\n",
      "Finished 700lines\n",
      "Finished 800lines\n",
      "Finished 900lines\n",
      "Finished 1000lines\n",
      "Finished 1100lines\n",
      "Finished 1200lines\n",
      "Finished 1300lines\n",
      "Finished 1400lines\n",
      "Finished 1500lines\n",
      "Finished 1600lines\n",
      "Finished 1700lines\n",
      "Finished 1800lines\n",
      "Finished 1900lines\n",
      "Finished 2000lines\n",
      "Finished 2100lines\n",
      "Finished 2200lines\n",
      "Finished 2300lines\n",
      "Finished 2400lines\n",
      "Finished 2500lines\n",
      "Finished 2600lines\n",
      "Finished 2700lines\n",
      "Finished 2800lines\n",
      "Finished 2900lines\n",
      "Finished 3000lines\n",
      "Finished 3100lines\n",
      "Finished 3200lines\n",
      "Finished 3300lines\n",
      "Finished 3400lines\n",
      "Finished 3500lines\n",
      "Finished 3600lines\n",
      "Finished 3700lines\n",
      "Finished 3800lines\n",
      "Finished 3900lines\n",
      "Finished 4000lines\n",
      "Finished 4100lines\n",
      "Finished 4200lines\n",
      "Finished 4300lines\n",
      "Finished 4400lines\n",
      "Finished 4500lines\n",
      "Finished 4600lines\n",
      "Finished 4700lines\n",
      "Finished 4800lines\n",
      "Finished 4900lines\n",
      "Finished 5000lines\n",
      "Finished 5100lines\n",
      "Finished 5200lines\n",
      "Finished 5300lines\n",
      "Finished 5400lines\n",
      "Finished 5500lines\n",
      "Finished 5600lines\n",
      "Finished 5700lines\n",
      "Finished 5800lines\n",
      "Finished 5900lines\n",
      "Finished 6000lines\n",
      "Finished 6100lines\n",
      "Finished 6200lines\n",
      "Finished 6300lines\n",
      "Finished 6400lines\n",
      "Finished 6500lines\n",
      "Finished 6600lines\n",
      "Finished 6700lines\n",
      "Finished 6800lines\n",
      "Finished 6900lines\n",
      "Finished 7000lines\n",
      "Finished 7100lines\n",
      "Finished 7200lines\n",
      "Finished 7300lines\n",
      "Finished 7400lines\n",
      "Finished 7500lines\n",
      "Finished 7600lines\n",
      "Finished 7700lines\n",
      "Finished 7800lines\n",
      "Finished 7900lines\n",
      "Finished 8000lines\n",
      "Finished 8100lines\n",
      "Finished 8200lines\n",
      "Finished 8300lines\n",
      "Finished 8400lines\n",
      "Finished 8500lines\n",
      "Finished 8600lines\n",
      "Finished 8700lines\n",
      "Finished 8800lines\n",
      "Finished 8900lines\n",
      "Finished 9000lines\n",
      "Finished 9100lines\n",
      "Finished 9200lines\n",
      "Finished 9300lines\n",
      "Finished 9400lines\n",
      "Finished 9500lines\n",
      "Finished 9600lines\n",
      "Finished 9700lines\n",
      "Finished 9800lines\n",
      "Finished 9900lines\n",
      "Finished 10000lines\n",
      "Finished 10100lines\n",
      "Finished 10200lines\n",
      "Finished 10300lines\n",
      "Finished 10400lines\n",
      "Finished 10500lines\n",
      "Finished 10600lines\n",
      "Finished 10700lines\n",
      "Finished 10800lines\n",
      "Finished 10900lines\n",
      "Finished 11000lines\n",
      "Finished 11100lines\n",
      "Finished 11200lines\n",
      "Finished 11300lines\n",
      "Finished 11400lines\n",
      "Finished 11500lines\n",
      "Finished 11600lines\n",
      "Finished 11700lines\n",
      "Finished 11800lines\n",
      "Finished 11900lines\n",
      "Finished 12000lines\n",
      "Finished 12100lines\n",
      "Finished 12200lines\n",
      "Finished 12300lines\n",
      "Finished 12400lines\n",
      "Finished 12500lines\n",
      "Finished 12600lines\n",
      "Finished 12700lines\n",
      "Finished 12800lines\n",
      "Finished 12900lines\n",
      "Finished 13000lines\n",
      "Finished 13100lines\n",
      "Finished 13200lines\n",
      "Finished 13300lines\n",
      "Finished 13400lines\n",
      "Finished 13500lines\n",
      "Finished 13600lines\n",
      "Finished 13700lines\n",
      "Finished 13800lines\n",
      "Finished 13900lines\n",
      "Finished 14000lines\n",
      "Finished 14100lines\n",
      "Finished 14200lines\n",
      "Finished 14300lines\n",
      "Finished 14400lines\n",
      "Finished 14500lines\n",
      "Finished 14600lines\n",
      "Finished 14700lines\n",
      "Finished 14800lines\n",
      "Finished 14900lines\n",
      "Finished 15000lines\n",
      "Finished 15100lines\n",
      "Finished 15200lines\n",
      "Finished 15300lines\n",
      "Finished 15400lines\n",
      "Finished 15500lines\n",
      "Finished 15600lines\n",
      "Finished 15700lines\n",
      "Finished 15800lines\n",
      "Finished 15900lines\n",
      "Finished 16000lines\n",
      "Finished 16100lines\n",
      "Finished 16200lines\n",
      "Finished 16300lines\n",
      "Finished 16400lines\n",
      "Finished 16500lines\n",
      "Finished 16600lines\n",
      "Finished 16700lines\n",
      "Finished 16800lines\n",
      "Finished 16900lines\n",
      "Finished 17000lines\n",
      "Finished 17100lines\n",
      "Finished 17200lines\n",
      "Finished 17300lines\n",
      "Finished 17400lines\n",
      "Finished 17500lines\n",
      "Finished 17600lines\n",
      "Finished 17700lines\n",
      "Finished 17800lines\n",
      "Finished 17900lines\n",
      "Finished 18000lines\n",
      "Finished 18100lines\n",
      "Finished 18200lines\n",
      "Finished 18300lines\n",
      "Finished 18400lines\n",
      "Finished 18500lines\n",
      "Finished 18600lines\n",
      "Finished 18700lines\n",
      "Finished 18800lines\n",
      "Finished 18900lines\n",
      "Finished 19000lines\n",
      "Finished 19100lines\n",
      "Finished 19200lines\n",
      "Finished 19300lines\n",
      "Finished 19400lines\n",
      "Finished 19500lines\n",
      "Finished 19600lines\n",
      "Finished 19700lines\n",
      "Finished 19800lines\n",
      "Finished 19900lines\n",
      "Finished 20000lines\n",
      "Finished 20100lines\n",
      "Finished 20200lines\n",
      "Finished 20300lines\n",
      "Finished 20400lines\n",
      "Finished 20500lines\n",
      "Finished 20600lines\n",
      "Finished 20700lines\n",
      "Finished 20800lines\n",
      "Finished 20900lines\n",
      "Finished 21000lines\n",
      "Finished 21100lines\n",
      "Finished 21200lines\n",
      "Finished 21300lines\n",
      "Finished 21400lines\n",
      "Finished 21500lines\n",
      "Finished 21600lines\n",
      "Finished 21700lines\n",
      "Finished 21800lines\n",
      "Finished 21900lines\n",
      "Finished 22000lines\n",
      "Finished 22100lines\n",
      "Finished 22200lines\n",
      "Finished 22300lines\n",
      "Finished 22400lines\n",
      "Finished 22500lines\n",
      "Finished 22600lines\n",
      "Finished 22700lines\n",
      "Finished 22800lines\n",
      "Finished 22900lines\n",
      "Finished 23000lines\n",
      "Finished 23100lines\n",
      "Finished 23200lines\n",
      "Finished 23300lines\n",
      "Finished 23400lines\n",
      "Finished 23500lines\n",
      "Finished 23600lines\n",
      "Finished 23700lines\n",
      "Finished 23800lines\n",
      "Finished 23900lines\n",
      "Finished 24000lines\n",
      "Finished 24100lines\n",
      "Finished 24200lines\n",
      "Finished 24300lines\n",
      "Finished 24400lines\n",
      "Finished 24500lines\n",
      "Finished 24600lines\n",
      "Finished 24700lines\n",
      "Finished 24800lines\n",
      "Finished 24900lines\n",
      "Finished 25000lines\n",
      "Finished 25100lines\n",
      "Finished 25200lines\n",
      "Finished 25300lines\n",
      "Finished 25400lines\n",
      "Finished 25500lines\n",
      "Finished 25600lines\n",
      "Finished 25700lines\n",
      "Finished 25800lines\n",
      "Finished 25900lines\n",
      "Finished 26000lines\n",
      "Finished 26100lines\n",
      "Finished 26200lines\n",
      "Finished 26300lines\n",
      "Finished 26400lines\n",
      "Finished 26500lines\n",
      "Finished 26600lines\n",
      "Finished 26700lines\n",
      "Finished 26800lines\n",
      "Finished 26900lines\n",
      "Finished 27000lines\n",
      "Finished 27100lines\n",
      "Finished 27200lines\n",
      "Finished 27300lines\n",
      "Finished 27400lines\n",
      "Finished 27500lines\n",
      "Finished 27600lines\n",
      "Finished 27700lines\n",
      "Finished 27800lines\n",
      "Finished 27900lines\n",
      "Finished 28000lines\n",
      "Finished 28100lines\n",
      "Finished 28200lines\n",
      "Finished 28300lines\n",
      "Finished 28400lines\n",
      "Finished 28500lines\n",
      "Finished 28600lines\n",
      "Finished 28700lines\n",
      "Finished 28800lines\n",
      "Finished 28900lines\n",
      "Finished 29000lines\n",
      "Finished 29100lines\n",
      "Finished 29200lines\n",
      "Finished 29300lines\n",
      "Finished 29400lines\n",
      "Finished 29500lines\n",
      "Finished 29600lines\n",
      "Finished 29700lines\n",
      "Finished 29800lines\n",
      "Finished 29900lines\n",
      "Finished 30000lines\n",
      "Finished 30100lines\n",
      "Finished 30200lines\n",
      "Finished 30300lines\n",
      "Finished 30400lines\n",
      "Finished 30500lines\n",
      "Finished 30600lines\n",
      "Finished 30700lines\n",
      "Finished 30800lines\n",
      "Finished 30900lines\n",
      "Finished 31000lines\n",
      "Finished 31100lines\n",
      "Finished 31200lines\n",
      "Finished 31300lines\n",
      "Finished 31400lines\n",
      "Finished 31500lines\n",
      "Finished 31600lines\n",
      "Finished 31700lines\n",
      "Finished 31800lines\n",
      "Finished 31900lines\n",
      "Finished 32000lines\n",
      "Finished 32100lines\n",
      "Finished 32200lines\n",
      "Finished 32300lines\n",
      "Finished 32400lines\n",
      "Finished 32500lines\n",
      "Finished 32600lines\n",
      "Finished 32700lines\n",
      "Finished 32800lines\n",
      "Finished 32900lines\n",
      "Finished 33000lines\n",
      "Finished 33100lines\n",
      "Finished 33200lines\n",
      "Finished 33300lines\n",
      "Finished 33400lines\n",
      "Finished 33500lines\n",
      "Finished 33600lines\n",
      "Finished 33700lines\n",
      "Finished 33800lines\n",
      "Finished 33900lines\n",
      "Finished 34000lines\n",
      "Finished 34100lines\n",
      "Finished 34200lines\n",
      "Finished 34300lines\n",
      "Finished 34400lines\n",
      "Finished 34500lines\n",
      "Finished 34600lines\n",
      "Finished 34700lines\n",
      "Finished 34800lines\n",
      "Finished 34900lines\n",
      "Finished 35000lines\n",
      "Finished 35100lines\n",
      "Finished 35200lines\n",
      "Finished 35300lines\n",
      "Finished 35400lines\n",
      "Finished 35500lines\n",
      "Finished 35600lines\n",
      "Finished 35700lines\n",
      "Finished 35800lines\n",
      "Finished 35900lines\n",
      "Finished 36000lines\n",
      "Finished 36100lines\n",
      "Finished 36200lines\n",
      "Finished 36300lines\n",
      "Finished 36400lines\n",
      "Finished 36500lines\n",
      "Finished 36600lines\n",
      "Finished 36700lines\n",
      "Finished 36800lines\n",
      "Finished 36900lines\n",
      "Finished 37000lines\n",
      "Finished 37100lines\n",
      "Finished 37200lines\n",
      "Finished 37300lines\n",
      "Finished 37400lines\n",
      "Finished 37500lines\n",
      "Finished 37600lines\n",
      "Finished 37700lines\n",
      "Finished 37800lines\n",
      "Finished 37900lines\n",
      "Finished 38000lines\n",
      "Finished 38100lines\n",
      "Finished 38200lines\n",
      "Finished 38300lines\n",
      "Finished 38400lines\n",
      "Finished 38500lines\n",
      "Finished 38600lines\n",
      "Finished 38700lines\n",
      "Finished 38800lines\n",
      "Finished 38900lines\n",
      "Finished 39000lines\n",
      "Finished 39100lines\n",
      "Finished 39200lines\n",
      "Finished 39300lines\n",
      "Finished 39400lines\n",
      "Finished 39500lines\n",
      "Finished 39600lines\n",
      "Finished 39700lines\n",
      "Finished 39800lines\n",
      "Finished 39900lines\n",
      "Finished 40000lines\n",
      "Finished 40100lines\n",
      "Finished 40200lines\n"
     ]
    }
   ],
   "source": [
    "if not isFile:\n",
    "    lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    for utterance in data:\n",
    "\n",
    "        if(count%100 ==0):\n",
    "            print('finished lemmetizing '+str(count)+' sentences')\n",
    "        count+=1\n",
    "\n",
    "        if utterance[4] != None or utterance[4] != 'Error':\n",
    "            text = regex.sub('', utterance[4])\n",
    "        else:\n",
    "            text = []\n",
    "    #     print(text)\n",
    "        lemmas = lemmer.lemmatize_text(text, return_pos=True)\n",
    "\n",
    "        lemmas = [a[0] for a in lemmas if len(a) == 2 and a[1]!=\"stopword\" and a[1]!=\"all\" and a[0] !='.']\n",
    "\n",
    "        data_utterance_lemmetization.append(lemmas)\n",
    "        for word in lemmas:\n",
    "            if word in dict_word_counts:\n",
    "                dict_word_counts[word] +=1\n",
    "            else:\n",
    "                dict_word_counts[word] = 1\n",
    "    i = 0\n",
    "    with open(\"Document_Frequencies.csv\", 'w',newline='') as csvfile: \n",
    "        header = ['Terms','Document_Count']\n",
    "        # creating a csv writer object \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "        csvwriter.writerow(header)\n",
    "        # writing the fields \n",
    "        for key in dict_word_counts:\n",
    "            if i%100 == 0:\n",
    "                print('finished writing '+str(i)+' setences on the file')\n",
    "            i+=1\n",
    "            row = []\n",
    "            row.append(key)\n",
    "            row.append(dict_word_counts[key])\n",
    "            csvwriter.writerow(row)\n",
    "    print(\"done\")\n",
    "    i = 0\n",
    "    with open(\"Document_Sentences.txt\", 'w',newline='') as file: \n",
    "        header = ['index','tokenization']\n",
    "        # creating a csv writer object \n",
    "        file.write('Index;Tokenization\\n')\n",
    "        # writing the fields \n",
    "        for sentence in data_utterance_lemmetization:\n",
    "            if i%100 == 0:\n",
    "                print('finished writing '+str(i)+' setences on the file')\n",
    "            row = str(i)+';'\n",
    "            for word in sentence:\n",
    "                row+=word+','\n",
    "\n",
    "            file.write(row+'\\n')  \n",
    "            i+=1\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"I am here\")\n",
    "    with open('Document_Frequencies.csv', 'r',encoding=\"utf8\") as reader:\n",
    "        lines = reader.readlines()\n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if(count%100 ==0):\n",
    "                print(\"Finished \"+str(count)+ \"lines\")\n",
    "            if(count != 0):\n",
    "                count_words = int(line.split(',')[1])       \n",
    "                dict_word_counts[line.split(',')[0]] = count_words\n",
    "            count += 1\n",
    "            \n",
    "    with open('Document_Sentences.txt', 'r',encoding=\"utf8\") as reader:\n",
    "        lines = reader.readlines()\n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if(count%100 ==0):\n",
    "                print(\"Finished \"+str(count)+ \"lines\")\n",
    "            if(count != 0):\n",
    "                without_line_breaks = line.replace(\"\\n\", \"\")\n",
    "                if len(line.split(';')[1])>1:\n",
    "                    without_line_breaks = without_line_breaks[:-1]\n",
    "                    data_utterance_lemmetization.append(without_line_breaks.split(';')[1].split(','))\n",
    "                else:\n",
    "                    data_utterance_lemmetization.append([])\n",
    "            count += 1\n",
    "#     LOAD FILES AND DEAL WITH THE ENCODINGS\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec7db21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تذكر',\n",
       " 'ذهاب',\n",
       " 'مشاهد',\n",
       " 'ألعاب',\n",
       " 'نار',\n",
       " 'عز',\n",
       " 'أصدقاء',\n",
       " 'مر',\n",
       " 'أولى',\n",
       " 'قضة',\n",
       " 'قت',\n",
       " 'مفرد',\n",
       " 'مع',\n",
       " 'رغم',\n",
       " 'جود',\n",
       " 'كثير',\n",
       " 'أشخاص',\n",
       " 'شعر',\n",
       " 'أشخاص',\n",
       " 'وحيد',\n",
       " 'عالم']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utterance_lemmetization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc3b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7558 40254 40254\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_word_counts),len(data_utterance_lemmetization),len(body)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7328976",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e77b6",
   "metadata": {},
   "source": [
    "## Get the total number of documents and remove the empty cells from the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7df409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n",
      "39729\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x in data_utterance_lemmetization:\n",
    "    if x ==[]:\n",
    "        counter+=1\n",
    "print(counter)\n",
    "total_number_documents = len(data_utterance_lemmetization)-counter\n",
    "print(total_number_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229f583",
   "metadata": {},
   "source": [
    "## make an array of normalized tf_idf for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a56390",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = []\n",
    "for sentence in data_utterance_lemmetization:\n",
    "    term_frequency = {}\n",
    "    tf_idf_words ={}\n",
    "    for word in sentence:\n",
    "        if word in term_frequency:\n",
    "            term_frequency[word] +=1\n",
    "        else:\n",
    "            term_frequency[word] = 1\n",
    "    for word in term_frequency:\n",
    "        tf_idf_words[word] = (1+math.log(term_frequency[word]))* (math.log(total_number_documents/dict_word_counts[word]))\n",
    "    normalization_value = 0\n",
    "    for word in tf_idf_words:\n",
    "        normalization_value += tf_idf_words[word]**2\n",
    "    normalization_value = math.sqrt(normalization_value)\n",
    "    \n",
    "    for word in tf_idf_words:\n",
    "        tf_idf_words[word] = tf_idf_words[word] / normalization_value\n",
    "        \n",
    "    tf_idf.append(tf_idf_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f178d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'تذكر': 0.19697797096916786,\n",
       " 'ذهاب': 0.17288693587530346,\n",
       " 'مشاهد': 0.21570034009722022,\n",
       " 'ألعاب': 0.23922837634965943,\n",
       " 'نار': 0.26346305705048984,\n",
       " 'عز': 0.2523956505305077,\n",
       " 'أصدقاء': 0.15869903565113733,\n",
       " 'مر': 0.1728274153034388,\n",
       " 'أولى': 0.21084671255004925,\n",
       " 'قضة': 0.26915997438471645,\n",
       " 'قت': 0.23451291417297054,\n",
       " 'مفرد': 0.23209987482781091,\n",
       " 'مع': 0.23140149570200128,\n",
       " 'رغم': 0.1820047139888703,\n",
       " 'جود': 0.2328097346713066,\n",
       " 'كثير': 0.12588548458957402,\n",
       " 'أشخاص': 0.3705680398067583,\n",
       " 'شعر': 0.09985551112189194,\n",
       " 'وحيد': 0.24268479400367984,\n",
       " 'عالم': 0.22517593156651244}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ba43a",
   "metadata": {},
   "source": [
    "## functions to process the query to the desired structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4e529c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(text):\n",
    "    lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    regex = re.compile(r'[\\',a-z,A-Z,0-9]+')\n",
    "    lemmas = lemmer.lemmatize_text(text, return_pos=True)\n",
    "    lemmas = [a[0] for a in lemmas if len(a) == 2 and a[1]!=\"stopword\" and a[1]!=\"all\" and a[0] !='.']\n",
    "    term_frequency = {}\n",
    "    tf_idf_words ={}\n",
    "    for word in lemmas:\n",
    "        if word in term_frequency:\n",
    "            term_frequency[word] +=1\n",
    "        else:\n",
    "            term_frequency[word] = 1\n",
    "            \n",
    "    for word in term_frequency:\n",
    "        tf_idf_words[word] = (1+math.log(term_frequency[word]))* (math.log(total_number_documents/dict_word_counts[word]))\n",
    "    normalization_value = 0\n",
    "    for word in tf_idf_words:\n",
    "        normalization_value += tf_idf_words[word]**2\n",
    "    normalization_value = math.sqrt(normalization_value)\n",
    "    \n",
    "    for word in tf_idf_words:\n",
    "        tf_idf_words[word] = tf_idf_words[word] / normalization_value\n",
    "    return tf_idf_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9661fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equality_context(x):\n",
    "    if x == \"معد\"or x == \"فخور\" or x == \"مخلص\"or x == \"عاطفي\" or x == \"ممتن\" or x == \"فرح\" or x == \"حنين\" or x == \"موثوق\" or x == \"الثقة\" or x == \"رعاية\" or x == \"متفائل\" or x == \"المحتوى\" or x == \"تأثرت\" or x == \"بهيجة\" or x == \"مندهش\" or x == \"توقع\":\n",
    "        return 1\n",
    "    return 0\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f34093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query,context):\n",
    "    text = process_query(query)\n",
    "    max_index = -1\n",
    "    max_similarity = 0\n",
    "    index = 0\n",
    "\n",
    "    for dictonary in tf_idf:\n",
    "        similarity = 0\n",
    "        for key in text:\n",
    "            if key in dictonary:\n",
    "                similarity+= text[key] *dictonary[key]\n",
    "        if similarity >max_similarity and equality_context(body[index][2]) == context :\n",
    "            max_index = index\n",
    "            max_similarity = similarity\n",
    "        index+=1\n",
    "    if max_index!= -1:\n",
    "        return max_similarity,body[max_index][3],body[max_index][2]\n",
    "    return 0,\"I couldn't hear you can you please say it again differently\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bd8ff4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'عاطفي'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c0474",
   "metadata": {},
   "source": [
    "## test string you can copy paste the string to try the chatbot... make sure to remove the string quations ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "011c0033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'أتذكر ذهابي لمشاهدة الألعاب النارية مع أعز أصدقائي. كانت هذه هي المرة الأولى التي قضينا فيها وقتًا بمفردنا معًا. على الرغم من وجود الكثير من الأشخاص ، إلا أننا شعرنا بأننا الأشخاص الوحيدون في العالم.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body[0][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5653fb",
   "metadata": {},
   "source": [
    "# Context neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b30333",
   "metadata": {},
   "source": [
    "## Processing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0c0b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9881677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(map(lambda x: 1 if x == \"معد\"or x == \"فخور\" or \n",
    "                      x == \"مخلص\"or x == \"عاطفي\" or x == \"ممتن\" or \n",
    "                      x == \"فرح\" or x == \"حنين\" or x == \"موثوق\" or \n",
    "                      x == \"الثقة\" or x == \"رعاية\" or x == \"متفائل\" or \n",
    "                      x == \"المحتوى\" or x == \"تأثرت\" or x == \"بهيجة\" or \n",
    "                      x == \"مندهش\" or x == \"توقع\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34a76cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in range(len(data_utterance_lemmetization)):\n",
    "    text.append(' '.join(data_utterance_lemmetization[i]))\n",
    "    \n",
    "\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6e1e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40254 40254\n"
     ]
    }
   ],
   "source": [
    "print(len(y),len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caff19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 7558\n",
    "vocab_size = 7558\n",
    "max_len = 25 \n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(text)\n",
    "text = tokenizer.texts_to_sequences(text)\n",
    "text = pad_sequences(text, padding='post', maxlen=max_len)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text, y, test_size =.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36100e",
   "metadata": {},
   "source": [
    "## Building the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd92eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(weights_path):\n",
    "    model = load_model(weights_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6962a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 25)          188950    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 15)                2460      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 32        \n",
      "=================================================================\n",
      "Total params: 191,442\n",
      "Trainable params: 191,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(layers.Embedding(max_words, 25)) #The embedding layer\n",
    "model1.add(layers.LSTM(15,dropout=0.5)) #Our LSTM layer\n",
    "model1.add(layers.Dense(2,activation='sigmoid'))\n",
    "\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "# model0 = Sequential()\n",
    "# model0.add(layers.Embedding(max_words, 15))\n",
    "# model0.add(layers.SimpleRNN(15,return_sequences=True))\n",
    "# model0.add(layers.SimpleRNN(15))\n",
    "# model0.add(layers.Dense(2,activation='softmax'))\n",
    "\n",
    "# model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b89a3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_found = os.path.isfile('best_model1.hdf5') \n",
    "\n",
    "if not model_found:\n",
    "    model1.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "    history = model1.fit(X_train, Y_train, epochs=10,validation_data=(X_test, Y_test),callbacks=[checkpoint1])\n",
    "else:\n",
    "    model1= keras.models.load_model(\"best_model1.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad4e9f",
   "metadata": {},
   "source": [
    "## Testing the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd6f2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.419205\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model1.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc52d9",
   "metadata": {},
   "source": [
    "# Integrating Similarity & NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed945879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_context_query(text):\n",
    "    lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    regex = re.compile(r'[\\',a-z,A-Z,0-9]+')\n",
    "    lemmas = lemmer.lemmatize_text(text, return_pos=True)\n",
    "    lemmas = [a[0] for a in lemmas if len(a) == 2 and a[1]!=\"stopword\" and a[1]!=\"all\" and a[0] !='.']\n",
    "    sentence = ' '.join(lemmas)\n",
    "    sentence = tokenizer.texts_to_sequences([sentence])\n",
    "    sentence = pad_sequences(sentence, padding='post', maxlen=max_len)\n",
    "    return sentence\n",
    "\n",
    "# query = process_context_query('لقد فقدت وظيفتي العام الماضي وغضبت حقًا.')  \n",
    "# y_pred = model1.predict(query)\n",
    "# query"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d88d6c0b",
   "metadata": {},
   "source": [
    "y_pred = np.argmax(y_pred)\n",
    "if(y_pred == 0):\n",
    "    print('negative emotion')\n",
    "else:\n",
    "    print('positive emotion')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07b9eb7b",
   "metadata": {},
   "source": [
    "y[14]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b87517fe",
   "metadata": {},
   "source": [
    "body[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee878e4",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42adaf",
   "metadata": {},
   "source": [
    "## run this cell for the chatbot and when you want to exit the chatbot write exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d67c047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "صباح الخير\n",
      "العمل رائع جدا ويحقق الكثير من المال!  the context is  موثوق\n",
      "كيف حالك؟\n",
      "أنا بخير على قدمي. أنا فقط بحاجة إلى القليل من المساعدة.  the context is  توقع\n",
      "كيف يمكنني أن أقدم المساعدة؟\n",
      "أنا سعيد لأنك تهتم بالآخرين.  the context is  رعاية\n",
      "اشكرك\n",
      "كل التوفيق لمستقبلك ودورك كذلك!  the context is  موثوق\n",
      "exit\n",
      "pleased to talk to you, goodbye\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    query = input()\n",
    "    if query != \"exit\":\n",
    "        processed_context_query = process_context_query(query)  \n",
    "        y_pred = model1.predict(processed_context_query)\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        similarity,answer,context = cosine_similarity(query,y_pred)\n",
    "        print(answer,' the context is ',context)\n",
    "    else:\n",
    "        print(\"pleased to talk to you, goodbye\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
